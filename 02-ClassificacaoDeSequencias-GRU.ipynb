{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmai_ryJVhd5"
   },
   "source": [
    "# Classificando nomes com uma *Character-Level RNN*\n",
    "\n",
    "Esse notebook foi criado com base no tutorial do PyTorch: <br> \n",
    "https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goO_pZBdVzrf"
   },
   "source": [
    "### Problema: Dado um nome próprio de entrada, classificar esse nome de acordo com a nacionalidade a que ele pertence.\n",
    "\n",
    "Entrada: **Hinton**\n",
    "\n",
    "(-0.47) Scottish\n",
    "\n",
    "(-1.52) English\n",
    "\n",
    "(-3.57) Irish\n",
    "\n",
    "\n",
    "-\n",
    "\n",
    "Entrada: **Schmidhuber**\n",
    "\n",
    "(-0.19) German\n",
    "\n",
    "(-2.48) Czech\n",
    "\n",
    "(-2.68) Dutch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lliVTIikWKQf"
   },
   "source": [
    "### Import de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4689,
     "status": "ok",
     "timestamp": 1605621847492,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "Y2lbiDXXUNGM"
   },
   "outputs": [],
   "source": [
    "# Importa utilitários para normalização/remoção de acentos e manipulação de caracteres Unicode.\n",
    "import unicodedata\n",
    "\n",
    "# Importa constantes e funções úteis para strings (pontuação, etc.).\n",
    "import string\n",
    "\n",
    "# Importa módulos de sistema, gerador aleatório e utilidades do sistema operacional.\n",
    "import sys, random, os\n",
    "\n",
    "\n",
    "# Importa o PyTorch para tensores e modelos.\n",
    "import torch\n",
    "\n",
    "# Do PyTorch, traz o submódulo nn (camadas, losses, etc.).\n",
    "from torch import nn\n",
    "\n",
    "# Importa o NumPy para operações numéricas com arrays.\n",
    "import numpy as np\n",
    "\n",
    "# Importa o Seaborn para melhorar a estética dos gráficos.\n",
    "import seaborn as sns\n",
    "\n",
    "# Define o estilo visual padrão dos gráficos do Seaborn.\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Importa o Matplotlib para plotagem.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# (Jupyter) Faz com que os gráficos sejam renderizados inline nas células.\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1605621850429,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "iIvPHo3Or62y"
   },
   "outputs": [],
   "source": [
    "# Cria um dicionário para hiperparâmetros e configs de treino/inferência.\n",
    "args = {\n",
    "# Taxa de aprendizado (learning rate) — aqui, 5×10⁻⁵.\n",
    "    'lr': 5e-5,\n",
    "# Fator de regularização L2 (weight decay) para evitar overfitting.\n",
    "    'regularizacao': 1e-7,\n",
    "# Número total de épocas de treinamento.\n",
    "    'num_epocas': 40,\n",
    "# Fecha o dicionário.\n",
    "}\n",
    "\n",
    "# (Opcional) Seleção automática do dispositivo: CUDA se disponível, senão CPU.\n",
    "# args['device'] = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Força o uso de CPU explicitamente (string 'cpu' também é aceita por torch.Tensor.to()).\n",
    "args['device'] = 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVssEJvAiaHp"
   },
   "source": [
    "## Dados de entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjtvooS3WsIB"
   },
   "source": [
    "### Importando dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1081,
     "status": "ok",
     "timestamp": 1605621868336,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "Smrb3GKqWsVu",
    "outputId": "ff368598-e30a-4194-9e97-f1d085b30acd"
   },
   "outputs": [],
   "source": [
    "# comente as duas linhas seguintes caso rode mais de uma vez\n",
    "# !wget https://download.pytorch.org/tutorial/data.zip #\n",
    "# !unzip data.zip #\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 768,
     "status": "ok",
     "timestamp": 1605621870665,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "xBgek-v83EA6",
    "outputId": "51268e25-0a73-4ede-a0af-452f76e7def5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Arabic.txt', 2000), ('Chinese.txt', 268), ('Czech.txt', 519), ('Dutch.txt', 297), ('English.txt', 3668), ('French.txt', 277), ('German.txt', 724), ('Greek.txt', 203), ('Irish.txt', 232), ('Italian.txt', 709), ('Japanese.txt', 991), ('Korean.txt', 94), ('Polish.txt', 139), ('Portuguese.txt', 74), ('Russian.txt', 9408), ('Scottish.txt', 100), ('Spanish.txt', 298), ('Vietnamese.txt', 73)]\n",
      "Minimo amostras ('Vietnamese.txt', 73) \n",
      "\n",
      "[b'Abreu', b'Albuquerque', b'Almeida', b'Alves', b'Araujo', b'Araullo', b'Barros', b'Basurto', b'Belo', b'Cabral']\n",
      "['Portuguese' 'Portuguese' 'Portuguese' 'Portuguese' 'Portuguese'\n",
      " 'Portuguese' 'Portuguese' 'Portuguese' 'Portuguese' 'Portuguese']\n"
     ]
    }
   ],
   "source": [
    "# Define uma função que lê um arquivo de nomes e retorna listas de nomes e rótulos.\n",
    "# Transforma um arquivo de nomes em listas e/ou arrays (nomes, rotulos)\n",
    "def readLines(filename):\n",
    "    # Lê o arquivo inteiro, remove espaços em branco nas extremidades e separa por quebras de linha.\n",
    "    lines     = open(filename).read().strip().split('\\n')\n",
    "    # Normaliza Unicode para ASCII (remove acentos); ATENÇÃO: o resultado aqui vira bytes (b'...').\n",
    "    nomes     = [unicodedata.normalize('NFKD', line).encode('ascii', 'ignore') for line in lines]\n",
    "    # Extrai a categoria a partir do nome do arquivo (ex.: 'Portuguese' de '.../Portuguese.txt').\n",
    "    categoria = filename.split('/')[-1].split('.')[0]\n",
    "    # Cria um vetor de rótulos repetindo a categoria para cada nome do arquivo.\n",
    "    rotulos   = np.repeat( categoria, len(nomes) )\n",
    "\n",
    "    # Retorna a lista de nomes normalizados e seus rótulos correspondentes.\n",
    "    return nomes, rotulos \n",
    "\n",
    "\n",
    "# Define o diretório onde estão os arquivos de nomes por idioma/categoria.\n",
    "root_path = 'data/names/'\n",
    "# Lista todos os arquivos do diretório, em ordem alfabética.\n",
    "arquivos = sorted(os.listdir(root_path))\n",
    "# Obtém a lista de categorias removendo a extensão '.txt' de cada arquivo.\n",
    "categorias = [a[:-4] for a in arquivos]\n",
    "\n",
    "# Inicializa contêineres para dados (nomes), rótulos e estatísticas por classe.\n",
    "dados, rotulos = [], []\n",
    "samples_perclass = []\n",
    "\n",
    "# Itera por cada arquivo de nomes disponível.\n",
    "for file_name in arquivos:\n",
    "  # Lê nomes e rótulos do arquivo atual.\n",
    "  retorno = readLines(os.path.join(root_path,file_name))\n",
    "  # Empilha a lista de nomes (bytes) na lista global 'dados'.\n",
    "  dados.append(retorno[0])\n",
    "  # Empilha o vetor de rótulos correspondente na lista global 'rotulos'.\n",
    "  rotulos.append(retorno[1])\n",
    "\n",
    "  # Armazena uma tupla (nome_do_arquivo, quantidade_de_nomes) para análise posterior.\n",
    "  samples_perclass.append( (file_name, len(retorno[0])) )\n",
    "\n",
    "\n",
    "# Imprime a contagem de amostras por arquivo/categoria.\n",
    "print(samples_perclass, )\n",
    "# Imprime qual categoria tem o menor número de amostras.\n",
    "print('Minimo amostras', min(samples_perclass, key= lambda k: k[1]), '\\n' )\n",
    "\n",
    "# Mostra os 10 primeiros nomes (em bytes) da categoria 'Portuguese'.\n",
    "print(dados[categorias.index('Portuguese')][0:10])\n",
    "# Mostra os 10 rótulos correspondentes a esses nomes.\n",
    "print(rotulos[categorias.index('Portuguese')][0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HlMtAzxfneB"
   },
   "source": [
    "### Convertendo os dados para tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMoQMcoRkSZd"
   },
   "source": [
    "**Convertendo os rótulos para tensor**\n",
    "\n",
    "Representação One-Hot de 18 categorias de idiomas que queremos prever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1605621873268,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "7HVlWioYjlUA",
    "outputId": "8d9d9345-b341-4485-b8b1-94c90ffcfa27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> Arabic tensor([0])\n"
     ]
    }
   ],
   "source": [
    "# Define uma função que converte um vetor de rótulos (strings) para um tensor de índices inteiros (shape N×1).\n",
    "def label2tensor(rotulos):\n",
    "  # Cria um tensor de zeros com N linhas e 1 coluna (tipo inteiro 64 bits) para armazenar os índices das classes.\n",
    "  rotulos_tns = torch.zeros( len(rotulos), 1, dtype=torch.int64 )\n",
    "  # Percorre cada rótulo e sua posição k.\n",
    "  for k, rotulo in enumerate(rotulos):\n",
    "    # Encontra o índice da classe na lista global 'categorias' (ex.: 'Portuguese' → índice correspondente).\n",
    "    idx = categorias.index(rotulo)\n",
    "    # Grava o índice encontrado na linha k (coluna 0) do tensor de rótulos.\n",
    "    rotulos_tns[k][0] = idx\n",
    "  # Retorna o tensor N×1 com os índices das classes.\n",
    "  return rotulos_tns\n",
    "\n",
    "# Seleciona o vetor de rótulos da primeira categoria listada (por exemplo, 'Arabic'), conforme a ordem em 'arquivos'.\n",
    "rotulos_arabe = rotulos[0]\n",
    "# Converte esse vetor de rótulos de strings para um tensor de índices inteiros.\n",
    "rotulos_tns = label2tensor(rotulos_arabe)\n",
    "# Imprime o tipo do tensor resultante, o primeiro rótulo original e o índice correspondente no tensor.\n",
    "print(type(rotulos_tns), rotulos_arabe[0], rotulos_tns[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JCS8K8XkWNQ"
   },
   "source": [
    "**Convertendo os nomes para tensor**\n",
    "\n",
    "Aqui também usaremos a representação One-Hot, porém teremos que trabalhar com uma lista de tensores, pois os nomes tem comprimentos diferentes. Mais à frente no curso aprenderemos a lidar com isso da forma certa!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1605621874458,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "ix07lvp9fmZi",
    "outputId": "0c745082-5590-407d-e6f8-82df615d018b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ '-\n",
      "K \n",
      " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0.])\n"
     ]
    }
   ],
   "source": [
    "# Define os caracteres válidos (apenas letras ASCII minúsculas e maiúsculas).\n",
    "caracteres_validos = string.ascii_letters + \" '-\"\n",
    "\n",
    "# Mostra o conjunto de caracteres válido (útil para checagem).\n",
    "print(caracteres_validos)\n",
    "\n",
    "# Tamanho do “vocabulário” de caracteres (número de colunas do one-hot).\n",
    "tam_dicionario = len(caracteres_validos)\n",
    "\n",
    "# Converte um nome (em bytes) para uma matriz one-hot de shape (len(nome), tam_dicionario).\n",
    "def nome2tensor(nome):\n",
    "    # Decodifica de bytes para string UTF-8 (ex.: b'José' → 'José').\n",
    "    nome = nome.decode('utf-8')\n",
    "    # Inicializa o tensor de zeros (uma linha por caractere, uma coluna por símbolo do vocabulário).\n",
    "    tns = torch.zeros(len(nome), tam_dicionario, dtype=torch.float32)\n",
    "    # Percorre cada caractere do nome e sua posição k.\n",
    "    for k, letra in enumerate(nome):\n",
    "        # Busca o índice da letra no vocabulário; se não existir, retorna -1.\n",
    "        idx = caracteres_validos.find(letra)\n",
    "        # Só marca 1.0 se a letra for conhecida (idx >= 0); evita escrever na coluna -1 por engano.\n",
    "        if idx >= 0:                 # << evita o índice -1\n",
    "            tns[k, idx] = 1.0\n",
    "        # Caso contrário, mantém a linha toda zero (caractere ignorado).\n",
    "        # else: linha fica toda zero (caractere ignorado)\n",
    "    # Retorna a matriz one-hot correspondente ao nome.\n",
    "    return tns\n",
    "\n",
    "# Seleciona a lista de nomes da primeira categoria (ex.: 'Arabic'), conforme a ordem em `arquivos`.\n",
    "dados_arabe = dados[0]\n",
    "\n",
    "# Converte cada nome dessa lista para seu tensor one-hot correspondente.\n",
    "dados_tns = [nome2tensor(dado) for dado in dados_arabe]\n",
    "\n",
    "# Imprime o primeiro caractere (já decodificado) e o vetor one-hot da primeira posição do primeiro nome.\n",
    "print(dados_arabe[0].decode('utf-8')[0],'\\n', dados_tns[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU4QqWqgBqxc"
   },
   "source": [
    "**Amostrando batch balanceado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1605621876555,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "GYfkmF8q5mT4",
    "outputId": "85b82f75-c2bd-4d6e-bda2-04192bfad5e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1118 torch.Size([6, 55]) torch.Size([1118, 1])\n"
     ]
    }
   ],
   "source": [
    "# Define o nº de amostras por classe como o mínimo entre as categorias (balanceamento por downsampling).\n",
    "num_amostras = min(samples_perclass, key= lambda k: k[1])[1]\n",
    "\n",
    "# Função que amostra, para cada categoria, 'size' nomes e retorna tensores dos nomes e rótulos.\n",
    "def sample_batch(size=num_amostras):\n",
    "  # Acumuladores de nomes (bytes) e rótulos (strings) do mini-conjunto balanceado.\n",
    "  dados_batch, rotulos_batch = [], []\n",
    "  # Percorre cada categoria disponível.\n",
    "  for cat in categorias:\n",
    "    \n",
    "    # Recupera a lista de nomes da categoria atual.\n",
    "    amostras_cat = dados[categorias.index(cat)]\n",
    "    # Escolhe 'size' índices aleatórios dessa categoria (por padrão, com reposição; use replace=False para sem reposição).\n",
    "    idx = np.random.choice(range(len(amostras_cat)), size=size)\n",
    "\n",
    "    # Coleta os nomes correspondentes aos índices sorteados e acumula.\n",
    "    dados_batch.extend([ r for k, r in enumerate(dados[categorias.index(cat)]) if k in idx])\n",
    "    # Coleta os rótulos correspondentes aos mesmos índices e acumula.\n",
    "    rotulos_batch.extend([ r for k, r in enumerate(rotulos[categorias.index(cat)]) if k in idx])\n",
    "\n",
    "  # Converte cada nome (bytes) para tensor one-hot (len(nome) × tam_dicionario).\n",
    "  dados_tns = [nome2tensor(dado) for dado in dados_batch]\n",
    "  # Converte os rótulos (strings) para tensor de índices (N × 1).\n",
    "  return dados_tns, label2tensor(rotulos_batch)\n",
    "\n",
    "# Gera um lote balanceado (mesmo nº por classe) e imprime algumas informações.\n",
    "dados_batch, rotulos_batch = sample_batch()\n",
    "print(len(dados_batch), dados_batch[0].size(), rotulos_batch.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por fim, vamos separar dados de treino e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa a função utilitária para dividir dados em treino e teste.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide os dados em conjuntos de treino e teste (80/20) com semente fixa para reprodutibilidade.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
    "    # Conjunto de entradas (lista de tensores one-hot por nome).\n",
    "    dados_tns,\n",
    "    # Conjunto de rótulos correspondentes (tensor N×1 com índices de classe).\n",
    "    rotulos_tns,\n",
    "    # Proporção destinada ao conjunto de teste: 20%.\n",
    "    test_size=0.2,\n",
    "    # Semente para tornar o split determinístico.\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnZL-mUXmMtA"
   },
   "source": [
    "## Modelo Recorrente\n",
    "\n",
    "* Implemente um modelo para classificação de nomes próprios (série de caracteres) usando apenas camadas *RNNCell*, *Linear* e ativação *LogSoftmax*\n",
    "* Cada entrada (caracter) possui dimensão (52): alfabeto maiúsculo e minúsculo\n",
    "* *Hidden size* possui dimensão (256): hiperparâmetro \n",
    "* Saída possui dimensão (18): vetor de probabilidade de classes\n",
    "* Batch size = 1 **pra não termos que lidar com as sequências de tamanho variável.**\n",
    "\n",
    "### Links úteis\n",
    "\n",
    "RNNCell: https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#torch.nn.RNNCell\n",
    "\n",
    "Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "\n",
    "Non-linear activations: https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1605622095577,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "PY9NvNN8kT-z"
   },
   "outputs": [],
   "source": [
    "# Define uma rede recorrente para classificar nomes em categorias usando GRU.\n",
    "class RNN(nn.Module):\n",
    "    # Construtor: recebe tamanhos de entrada (one-hot), oculto (features) e saída (nº de classes).\n",
    "    def __init__(self, tam_entrada, tam_feature, tam_saida):\n",
    "        # Inicializa a superclasse nn.Module.\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # Guarda os hiperparâmetros como atributos do módulo.\n",
    "        self.tam_entrada = tam_entrada\n",
    "        self.tam_feature = tam_feature\n",
    "        self.tam_saida   = tam_saida\n",
    "        \n",
    "        # Célula recorrente GRU processando sequências com batch_first=True → entrada (B, T, C).\n",
    "        self.rnn    = nn.GRU(self.tam_entrada, self.tam_feature, batch_first=True)\n",
    "        # Camada linear mapeando o estado oculto final para logits de classes.\n",
    "        self.linear = nn.Linear(self.tam_feature, self.tam_saida)\n",
    "        # Converte logits em log-probabilidades por classe (última dimensão).\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    # Define o fluxo direto: recebe um tensor (T, C) de um nome em one-hot.\n",
    "    def forward(self, nome):\n",
    "      \n",
    "        # Cria o estado oculto inicial da GRU com zeros: (num_layers, batch, hidden_size) = (1, 1, tam_feature).\n",
    "        batch_size = 1\n",
    "        hidden = torch.zeros(1, batch_size, self.tam_feature).to(args['device'])\n",
    "        \n",
    "        # Adiciona dimensão de batch (1) para casar com batch_first=True → (1, T, C).\n",
    "        # nome.unsqueeze(0) - criando uma dimensão artificial para o batch = 1\n",
    "        saida, hidden = self.rnn(nome.unsqueeze(0), hidden)\n",
    "        # Seleciona apenas a última saída temporal (estado do último caractere) com shape (1, tam_feature).\n",
    "        # print(\"Saída da GRU:\", saida.size())\n",
    "        saida = self.linear(saida[:, -1])\n",
    "        # Projeta para o espaço de classes → (1, tam_saida).\n",
    "        # print(\"Saída da Linear:\", saida.size())\n",
    "        saida = self.softmax(saida) \n",
    "        # Retorna log-probabilidades (1, tam_saida).\n",
    "        return saida\n",
    "\n",
    "# Define o tamanho do estado oculto da GRU (número de features internas).\n",
    "tam_feature = 256\n",
    "# Instancia o modelo com entrada = tamanho do dicionário, saída = número de categorias.\n",
    "model = RNN(tam_dicionario, tam_feature, len(categorias))\n",
    "# Move o modelo para o dispositivo definido em args['device'] (CPU/GPU).\n",
    "model.to(args['device'])\n",
    "\n",
    "# Cria um exemplo sintético: sequência de 8 caracteres, vocabulário de 52 (one-hot zero por enquanto).\n",
    "nome = torch.zeros(8, 55).to(args['device'])\n",
    "# Executa uma passada de inferência para verificar o fluxo e as dimensões.\n",
    "saida = model(nome)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVSrUbUns5WH"
   },
   "source": [
    "## Loss e Otimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1605622099370,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "0UwVsprXr5BV"
   },
   "outputs": [],
   "source": [
    "# Define a função de perda NLL (compatível com saídas em log-probabilidade do LogSoftmax) e move para o device.\n",
    "criterion = nn.NLLLoss().to(args['device']) \n",
    "\n",
    "# Cria o otimizador Adam para os parâmetros do modelo, com taxa de aprendizado e regularização L2 definidas em args.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['regularizacao'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mNex6BA1s4_W"
   },
   "source": [
    "## Treinamento\n",
    "\n",
    "A otimização é um processo que tem uma raíz muito bem definida de passo a passo, sempre fazemos:\n",
    "* Carregar os dados e colocar no dispositivo de hardware adequado\n",
    "* Forward do dado na rede\n",
    "* Cálculo da função de custo (no nosso caso uma função composta)\n",
    "* Passos de Otimização\n",
    "  * Zerar os gradientes do otimizador (`optimizer.zero_grad()`)\n",
    "  * Calcular os gradientes com base na loss (`loss.backward()`)\n",
    "  * Passo de otimização (`optimizer.step()`)\n",
    "\n",
    "Apesar de cada solução ter pequenas variações em um ou mais passos do fluxo, o esqueleto é sempre o mesmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 858,
     "status": "ok",
     "timestamp": 1605622106467,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "N3AvGhzMscZT"
   },
   "outputs": [],
   "source": [
    "# Define uma passada sobre os dados X/Y; alterna entre treino e avaliação conforme `etapa`.\n",
    "def forward(X, Y, etapa):\n",
    "\n",
    "  # Coloca o modelo em modo de treino (Dropout/BN em training) ou avaliação (eval).\n",
    "  if etapa == 'Treino': model.train()\n",
    "  else: model.eval()\n",
    "\n",
    "  # Inicializa acumuladores de acurácia e lista de perdas por amostra.\n",
    "  acuracia = 0.\n",
    "  loss_epoca = []\n",
    "\n",
    "  # Itera sincronicamente por amostra (dado) e rótulo (rotulo).\n",
    "  for k, (dado, rotulo) in enumerate(zip(X, Y)):\n",
    "      \n",
    "    # Move entrada e rótulo para o dispositivo configurado (CPU/GPU).\n",
    "    dado = dado.to(args['device'])\n",
    "    rotulo = rotulo.to(args['device'])\n",
    "    \n",
    "    # Forward: produz as pontuações/log-probabilidades do modelo.\n",
    "    saida = model(dado)\n",
    "    # Calcula a perda da amostra segundo o critério escolhido (ex.: NLLLoss).\n",
    "    loss = criterion(saida, rotulo)\n",
    "    # Armazena o valor escalar da perda (sem gradiente) para estatísticas.\n",
    "    loss_epoca.append(loss.detach().cpu().numpy())\n",
    "\n",
    "    # Obtém a classe prevista (índice do maior valor na última dimensão).\n",
    "    _, pred = torch.max(saida, axis=-1)\n",
    "    # Soma 1 acerto se a predição coincide com o rótulo verdadeiro.\n",
    "    acuracia += 1 if pred[0].item() == rotulo[0].item() else 0\n",
    "\n",
    "    # Se estiver em etapa de treino, faz backprop e atualiza os pesos.\n",
    "    if etapa == 'Treino':\n",
    "      # Zera gradientes acumulados do passo anterior.\n",
    "      # Otimização\n",
    "      optimizer.zero_grad()\n",
    "      # Backpropaga o erro.\n",
    "      loss.backward()\n",
    "      # Aplica a atualização de parâmetros.\n",
    "      optimizer.step()\n",
    "\n",
    "  # Converte lista de perdas para NumPy e achata para facilitar estatísticas.\n",
    "  loss_epoca = np.asarray(loss_epoca).ravel()\n",
    "  # Converte acertos absolutos em fração de acertos (acurácia em [0,1]).\n",
    "  acuracia   = acuracia/float(len(loss_epoca))\n",
    "\n",
    "  # Cabeçalho de log amigável para diferenciar TREINO/TESTE.\n",
    "  print('\\n','*'*15 + etapa + '*'*15 )\n",
    "  # Exibe média±desvio da loss e acurácia; OBS: usa variável global `epoca`.\n",
    "  print('Epoca: {:}, Loss: {:.4f} +/- {:.4f}, Acurácia: {:.4f}'.format(epoca, loss_epoca.mean(), \n",
    "                                                                        loss_epoca.std(), \n",
    "                                                                        acuracia\n",
    "                                                                       )) \n",
    "  # Retorna a perda média da época e a acurácia correspondente.\n",
    "  return loss_epoca.mean(), acuracia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 673072,
     "status": "ok",
     "timestamp": 1605622782199,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "RaxYaVprzIS3",
    "outputId": "8d724eb0-2033-4539-ec83-bfbdff31c614"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ***************Treino***************\n",
      "Epoca: 0, Loss: 2.8984 +/- 0.2045, Acurácia: 0.0596\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 0, Loss: 2.9100 +/- 0.2093, Acurácia: 0.0568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 1, Loss: 2.8061 +/- 0.4255, Acurácia: 0.0448\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 1, Loss: 3.0009 +/- 0.6590, Acurácia: 0.0568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 2, Loss: 2.8486 +/- 0.3993, Acurácia: 0.0634\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 2, Loss: 2.9399 +/- 0.7595, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 3, Loss: 2.8178 +/- 0.4005, Acurácia: 0.1107\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 3, Loss: 3.0176 +/- 0.9070, Acurácia: 0.0682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 4, Loss: 2.8377 +/- 0.3204, Acurácia: 0.1005\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 4, Loss: 3.0001 +/- 0.8496, Acurácia: 0.0682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 5, Loss: 2.8416 +/- 0.2695, Acurácia: 0.1110\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 5, Loss: 2.8753 +/- 0.4962, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 6, Loss: 2.7598 +/- 0.3936, Acurácia: 0.1349\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 6, Loss: 3.0882 +/- 0.9650, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 7, Loss: 2.7600 +/- 0.3726, Acurácia: 0.1413\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 7, Loss: 2.9009 +/- 0.6219, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 8, Loss: 2.7103 +/- 0.4170, Acurácia: 0.1434\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 8, Loss: 2.7990 +/- 0.2394, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 9, Loss: 2.6191 +/- 0.4810, Acurácia: 0.1458\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 9, Loss: 2.7949 +/- 0.4284, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 10, Loss: 2.5883 +/- 0.5319, Acurácia: 0.1546\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 10, Loss: 2.9514 +/- 0.7120, Acurácia: 0.0682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 11, Loss: 2.5160 +/- 0.6616, Acurácia: 0.1763\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 11, Loss: 3.0942 +/- 0.8597, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 12, Loss: 2.4704 +/- 0.6802, Acurácia: 0.1708\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 12, Loss: 2.9932 +/- 0.8701, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 13, Loss: 2.3220 +/- 0.8298, Acurácia: 0.2440\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 13, Loss: 3.1115 +/- 1.1021, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 14, Loss: 2.2617 +/- 0.8590, Acurácia: 0.2587\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 14, Loss: 3.1427 +/- 1.1841, Acurácia: 0.0682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 15, Loss: 2.1051 +/- 0.8896, Acurácia: 0.2889\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 15, Loss: 3.1748 +/- 1.2406, Acurácia: 0.0682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 16, Loss: 2.0596 +/- 0.9157, Acurácia: 0.2975\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 16, Loss: 3.2378 +/- 1.2771, Acurácia: 0.0568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 17, Loss: 2.0323 +/- 0.9707, Acurácia: 0.3252\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 17, Loss: 3.2375 +/- 1.2733, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 18, Loss: 1.9520 +/- 0.9929, Acurácia: 0.3247\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 18, Loss: 3.3355 +/- 1.3737, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 19, Loss: 1.8885 +/- 1.0323, Acurácia: 0.3482\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 19, Loss: 3.4118 +/- 1.4957, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 20, Loss: 1.8728 +/- 1.0859, Acurácia: 0.3563\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 20, Loss: 3.5770 +/- 1.6733, Acurácia: 0.0909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 21, Loss: 1.8567 +/- 1.0460, Acurácia: 0.3646\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 21, Loss: 3.7341 +/- 1.8112, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 22, Loss: 1.8495 +/- 1.0764, Acurácia: 0.3540\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 22, Loss: 3.7653 +/- 1.8340, Acurácia: 0.0909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 23, Loss: 1.7818 +/- 1.0298, Acurácia: 0.3702\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 23, Loss: 3.8929 +/- 1.9169, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 24, Loss: 1.8014 +/- 1.1517, Acurácia: 0.3815\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 24, Loss: 3.9565 +/- 2.0117, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 25, Loss: 1.7268 +/- 1.1065, Acurácia: 0.3956\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 25, Loss: 3.9479 +/- 2.0377, Acurácia: 0.0909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 26, Loss: 1.6763 +/- 1.1335, Acurácia: 0.4480\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 26, Loss: 4.0185 +/- 2.1142, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 27, Loss: 1.6899 +/- 1.1626, Acurácia: 0.4429\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 27, Loss: 3.9441 +/- 2.0718, Acurácia: 0.0909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 28, Loss: 1.7009 +/- 1.1845, Acurácia: 0.4350\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 28, Loss: 3.8737 +/- 2.0013, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 29, Loss: 1.7538 +/- 1.1806, Acurácia: 0.4077\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 29, Loss: 3.9102 +/- 2.0441, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 30, Loss: 1.6868 +/- 1.1824, Acurácia: 0.4315\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 30, Loss: 3.7536 +/- 1.9403, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 31, Loss: 1.7260 +/- 1.1905, Acurácia: 0.4240\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 31, Loss: 3.8143 +/- 2.0183, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 32, Loss: 1.7354 +/- 1.2146, Acurácia: 0.4128\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 32, Loss: 3.8476 +/- 2.0207, Acurácia: 0.0909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 33, Loss: 1.7046 +/- 1.1986, Acurácia: 0.4129\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 33, Loss: 3.8559 +/- 2.0568, Acurácia: 0.0568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 34, Loss: 1.7105 +/- 1.1975, Acurácia: 0.4066\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 34, Loss: 3.9155 +/- 2.1015, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 35, Loss: 1.6979 +/- 1.2179, Acurácia: 0.4259\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 35, Loss: 3.8215 +/- 2.0267, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 36, Loss: 1.6999 +/- 1.2123, Acurácia: 0.4193\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 36, Loss: 3.8591 +/- 2.0422, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 37, Loss: 1.7049 +/- 1.2165, Acurácia: 0.4315\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 37, Loss: 3.6093 +/- 1.8521, Acurácia: 0.0909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 38, Loss: 1.7333 +/- 1.2113, Acurácia: 0.4147\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 38, Loss: 3.6829 +/- 1.9576, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 39, Loss: 1.7036 +/- 1.2043, Acurácia: 0.4201\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 39, Loss: 3.9012 +/- 2.1579, Acurácia: 0.0682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 40, Loss: 1.6779 +/- 1.2109, Acurácia: 0.4386\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 40, Loss: 3.8883 +/- 2.2029, Acurácia: 0.0795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 41, Loss: 1.6662 +/- 1.2119, Acurácia: 0.4522\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 41, Loss: 4.1888 +/- 2.4308, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 42, Loss: 1.6487 +/- 1.2011, Acurácia: 0.4448\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 42, Loss: 4.1541 +/- 2.3843, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 43, Loss: 1.6337 +/- 1.1793, Acurácia: 0.4513\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 43, Loss: 3.9862 +/- 2.2764, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 44, Loss: 1.7176 +/- 1.1527, Acurácia: 0.4107\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 44, Loss: 3.9006 +/- 2.1714, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 45, Loss: 1.7598 +/- 1.2129, Acurácia: 0.4379\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 45, Loss: 3.8473 +/- 2.0836, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 46, Loss: 1.7279 +/- 1.2057, Acurácia: 0.4352\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 46, Loss: 3.9194 +/- 2.1223, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 47, Loss: 1.7120 +/- 1.2108, Acurácia: 0.4351\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 47, Loss: 3.8401 +/- 2.0379, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 48, Loss: 1.6249 +/- 1.1960, Acurácia: 0.4553\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 48, Loss: 3.8284 +/- 2.0492, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 49, Loss: 1.6557 +/- 1.1772, Acurácia: 0.4379\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 49, Loss: 3.7081 +/- 1.9021, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 50, Loss: 1.5957 +/- 1.1695, Acurácia: 0.4671\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 50, Loss: 3.8004 +/- 1.9941, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 51, Loss: 1.6296 +/- 1.1744, Acurácia: 0.4540\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 51, Loss: 3.7164 +/- 1.9388, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 52, Loss: 1.5908 +/- 1.1763, Acurácia: 0.4596\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 52, Loss: 3.7231 +/- 1.9345, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 53, Loss: 1.6383 +/- 1.1555, Acurácia: 0.4293\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 53, Loss: 3.6707 +/- 1.8571, Acurácia: 0.1364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 54, Loss: 1.6477 +/- 1.1666, Acurácia: 0.4333\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 54, Loss: 3.5342 +/- 1.7148, Acurácia: 0.1364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 55, Loss: 1.7258 +/- 1.1915, Acurácia: 0.4319\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 55, Loss: 3.6025 +/- 1.8431, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 56, Loss: 1.6503 +/- 1.1502, Acurácia: 0.4528\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 56, Loss: 3.5067 +/- 1.7032, Acurácia: 0.1364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 57, Loss: 1.6450 +/- 1.1663, Acurácia: 0.4307\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 57, Loss: 3.2569 +/- 1.4666, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 58, Loss: 1.6462 +/- 1.1664, Acurácia: 0.4401\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 58, Loss: 3.3904 +/- 1.6741, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 59, Loss: 1.6800 +/- 1.1512, Acurácia: 0.4280\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 59, Loss: 3.4706 +/- 1.7877, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 60, Loss: 1.6190 +/- 1.1340, Acurácia: 0.4464\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 60, Loss: 3.3140 +/- 1.6419, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 61, Loss: 1.6435 +/- 1.1188, Acurácia: 0.4301\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 61, Loss: 3.2204 +/- 1.5525, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 62, Loss: 1.6484 +/- 1.1087, Acurácia: 0.4208\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 62, Loss: 3.2154 +/- 1.5724, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 63, Loss: 1.6625 +/- 1.0918, Acurácia: 0.4092\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 63, Loss: 3.3109 +/- 1.7181, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 64, Loss: 1.5922 +/- 1.0909, Acurácia: 0.4588\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 64, Loss: 3.4184 +/- 1.8375, Acurácia: 0.1364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 65, Loss: 1.6609 +/- 1.0971, Acurácia: 0.4240\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 65, Loss: 3.7128 +/- 2.1113, Acurácia: 0.1364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 66, Loss: 1.6186 +/- 1.0687, Acurácia: 0.4441\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 66, Loss: 3.9248 +/- 2.3189, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 67, Loss: 1.6332 +/- 1.0873, Acurácia: 0.4297\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 67, Loss: 3.6833 +/- 2.1595, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 68, Loss: 1.6510 +/- 1.1046, Acurácia: 0.4462\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 68, Loss: 3.5229 +/- 1.9858, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 69, Loss: 1.6464 +/- 1.1207, Acurácia: 0.4377\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 69, Loss: 3.4835 +/- 1.9664, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 70, Loss: 1.5703 +/- 1.1041, Acurácia: 0.4693\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 70, Loss: 3.6266 +/- 2.1544, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 71, Loss: 1.6098 +/- 1.0818, Acurácia: 0.4458\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 71, Loss: 3.4462 +/- 1.9784, Acurácia: 0.1364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 72, Loss: 1.6119 +/- 1.0989, Acurácia: 0.4536\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 72, Loss: 3.4265 +/- 1.9604, Acurácia: 0.1023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 73, Loss: 1.5765 +/- 1.0500, Acurácia: 0.4545\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 73, Loss: 3.4841 +/- 2.0470, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 74, Loss: 1.5820 +/- 1.0745, Acurácia: 0.4545\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 74, Loss: 3.5195 +/- 2.1246, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 75, Loss: 1.5793 +/- 1.1179, Acurácia: 0.4650\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 75, Loss: 3.6287 +/- 2.2911, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 76, Loss: 1.5746 +/- 1.1146, Acurácia: 0.4729\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 76, Loss: 3.7223 +/- 2.2836, Acurácia: 0.0909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 77, Loss: 1.5288 +/- 1.1346, Acurácia: 0.4910\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 77, Loss: 3.5585 +/- 2.1113, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 78, Loss: 1.5596 +/- 1.1452, Acurácia: 0.4938\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 78, Loss: 3.5845 +/- 2.0991, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 79, Loss: 1.5159 +/- 1.1510, Acurácia: 0.5027\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 79, Loss: 3.5512 +/- 2.0476, Acurácia: 0.1136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 80, Loss: 1.4973 +/- 1.1073, Acurácia: 0.4938\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 80, Loss: 3.5195 +/- 2.0319, Acurácia: 0.1250\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 81, Loss: 1.5235 +/- 1.1180, Acurácia: 0.4823\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 81, Loss: 3.3389 +/- 1.9008, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 82, Loss: 1.5246 +/- 1.1292, Acurácia: 0.4846\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 82, Loss: 3.2397 +/- 1.8326, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 83, Loss: 1.5120 +/- 1.1363, Acurácia: 0.4973\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 83, Loss: 3.1177 +/- 1.7139, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 84, Loss: 1.4844 +/- 1.1244, Acurácia: 0.5084\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 84, Loss: 3.5141 +/- 2.0652, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 85, Loss: 1.4225 +/- 1.1030, Acurácia: 0.5191\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 85, Loss: 3.4354 +/- 2.0614, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 86, Loss: 1.4795 +/- 1.0722, Acurácia: 0.5013\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 86, Loss: 3.5041 +/- 2.1373, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 87, Loss: 1.4602 +/- 1.1185, Acurácia: 0.5162\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 87, Loss: 3.2570 +/- 1.8924, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 88, Loss: 1.4875 +/- 1.0963, Acurácia: 0.5053\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 88, Loss: 3.3306 +/- 1.9948, Acurácia: 0.1818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 89, Loss: 1.4808 +/- 1.1138, Acurácia: 0.5018\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 89, Loss: 3.4346 +/- 2.1521, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 90, Loss: 1.4984 +/- 1.1241, Acurácia: 0.4853\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 90, Loss: 3.4985 +/- 2.2152, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 91, Loss: 1.5234 +/- 1.1184, Acurácia: 0.4736\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 91, Loss: 3.2945 +/- 2.0495, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 92, Loss: 1.5085 +/- 1.1343, Acurácia: 0.4925\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 92, Loss: 3.1300 +/- 1.8367, Acurácia: 0.1818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 93, Loss: 1.5396 +/- 1.1427, Acurácia: 0.4853\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 93, Loss: 3.3056 +/- 1.9888, Acurácia: 0.1477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 94, Loss: 1.5284 +/- 1.1446, Acurácia: 0.4951\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 94, Loss: 3.3260 +/- 2.0632, Acurácia: 0.1818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 95, Loss: 1.4780 +/- 1.1366, Acurácia: 0.5119\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 95, Loss: 3.4679 +/- 2.2595, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 96, Loss: 1.5314 +/- 1.1182, Acurácia: 0.4908\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 96, Loss: 3.6496 +/- 2.4796, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 97, Loss: 1.5012 +/- 1.1215, Acurácia: 0.5026\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 97, Loss: 3.5963 +/- 2.5000, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 98, Loss: 1.5629 +/- 1.1120, Acurácia: 0.4782\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 98, Loss: 3.4518 +/- 2.3613, Acurácia: 0.2386\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 99, Loss: 1.4930 +/- 1.1079, Acurácia: 0.5045\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 99, Loss: 3.5920 +/- 2.5177, Acurácia: 0.2273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 100, Loss: 1.4756 +/- 1.1265, Acurácia: 0.4996\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 100, Loss: 3.5401 +/- 2.4499, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 101, Loss: 1.4627 +/- 1.0928, Acurácia: 0.4982\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 101, Loss: 3.4463 +/- 2.3430, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 102, Loss: 1.4718 +/- 1.1007, Acurácia: 0.5030\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 102, Loss: 3.4032 +/- 2.2697, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 103, Loss: 1.4553 +/- 1.1081, Acurácia: 0.5022\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 103, Loss: 3.2158 +/- 2.0062, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 104, Loss: 1.4941 +/- 1.1254, Acurácia: 0.5031\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 104, Loss: 3.4096 +/- 2.2208, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 105, Loss: 1.4657 +/- 1.1162, Acurácia: 0.5049\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 105, Loss: 3.4278 +/- 2.2505, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 106, Loss: 1.4279 +/- 1.0988, Acurácia: 0.5102\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 106, Loss: 3.5869 +/- 2.4111, Acurácia: 0.1818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 107, Loss: 1.4433 +/- 1.1326, Acurácia: 0.5181\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 107, Loss: 3.5725 +/- 2.4381, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 108, Loss: 1.4493 +/- 1.1196, Acurácia: 0.5179\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 108, Loss: 3.5997 +/- 2.5041, Acurácia: 0.1591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 109, Loss: 1.4442 +/- 1.1419, Acurácia: 0.5297\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 109, Loss: 3.4023 +/- 2.3041, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 110, Loss: 1.4520 +/- 1.1416, Acurácia: 0.5058\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 110, Loss: 3.3578 +/- 2.2596, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 111, Loss: 1.4635 +/- 1.1348, Acurácia: 0.5158\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 111, Loss: 3.4183 +/- 2.2691, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 112, Loss: 1.4530 +/- 1.1173, Acurácia: 0.5054\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 112, Loss: 3.2143 +/- 2.0724, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 113, Loss: 1.3988 +/- 1.1070, Acurácia: 0.5321\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 113, Loss: 3.3196 +/- 2.2016, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 114, Loss: 1.4442 +/- 1.1438, Acurácia: 0.5230\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 114, Loss: 3.3197 +/- 2.2455, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 115, Loss: 1.4347 +/- 1.1267, Acurácia: 0.5264\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 115, Loss: 3.3024 +/- 2.2260, Acurácia: 0.2273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 116, Loss: 1.4435 +/- 1.1417, Acurácia: 0.5232\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 116, Loss: 3.3322 +/- 2.2707, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 117, Loss: 1.4598 +/- 1.1371, Acurácia: 0.5196\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 117, Loss: 3.1443 +/- 2.0542, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 118, Loss: 1.4668 +/- 1.1476, Acurácia: 0.5171\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 118, Loss: 3.1797 +/- 2.1237, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 119, Loss: 1.4502 +/- 1.1584, Acurácia: 0.5251\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 119, Loss: 3.1440 +/- 2.0425, Acurácia: 0.1705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 120, Loss: 1.4376 +/- 1.0977, Acurácia: 0.5053\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 120, Loss: 3.0855 +/- 1.9878, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 121, Loss: 1.4579 +/- 1.0936, Acurácia: 0.5000\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 121, Loss: 3.0481 +/- 1.9572, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 122, Loss: 1.4028 +/- 1.1437, Acurácia: 0.5446\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 122, Loss: 2.9848 +/- 1.9175, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 123, Loss: 1.4055 +/- 1.1017, Acurácia: 0.5370\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 123, Loss: 2.8117 +/- 1.6756, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 124, Loss: 1.4113 +/- 1.0998, Acurácia: 0.5310\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 124, Loss: 2.8091 +/- 1.7160, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 125, Loss: 1.3960 +/- 1.1032, Acurácia: 0.5388\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 125, Loss: 2.9049 +/- 1.8105, Acurácia: 0.1818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 126, Loss: 1.4185 +/- 1.0969, Acurácia: 0.5327\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 126, Loss: 3.0889 +/- 2.0719, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 127, Loss: 1.4085 +/- 1.1041, Acurácia: 0.5286\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 127, Loss: 3.1992 +/- 2.1792, Acurácia: 0.1818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 128, Loss: 1.4208 +/- 1.1188, Acurácia: 0.5362\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 128, Loss: 3.0925 +/- 2.0988, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 129, Loss: 1.4301 +/- 1.1408, Acurácia: 0.5398\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 129, Loss: 2.9842 +/- 2.0022, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 130, Loss: 1.3673 +/- 1.0698, Acurácia: 0.5539\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 130, Loss: 2.9003 +/- 1.9075, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 131, Loss: 1.3922 +/- 1.1007, Acurácia: 0.5523\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 131, Loss: 2.8423 +/- 1.8777, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 132, Loss: 1.3504 +/- 1.1184, Acurácia: 0.5532\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 132, Loss: 3.0312 +/- 2.0470, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 133, Loss: 1.3255 +/- 1.1092, Acurácia: 0.5521\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 133, Loss: 3.1971 +/- 2.2610, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 134, Loss: 1.3817 +/- 1.1701, Acurácia: 0.5568\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 134, Loss: 3.0729 +/- 2.1300, Acurácia: 0.1932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 135, Loss: 1.3839 +/- 1.1563, Acurácia: 0.5533\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 135, Loss: 2.9995 +/- 2.0651, Acurácia: 0.2273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 136, Loss: 1.3600 +/- 1.1298, Acurácia: 0.5698\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 136, Loss: 3.0152 +/- 2.1011, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 137, Loss: 1.3260 +/- 1.0818, Acurácia: 0.5484\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 137, Loss: 3.0350 +/- 2.0856, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 138, Loss: 1.4441 +/- 1.1733, Acurácia: 0.5183\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 138, Loss: 2.8592 +/- 1.9216, Acurácia: 0.2045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 139, Loss: 1.3538 +/- 1.1151, Acurácia: 0.5553\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 139, Loss: 2.8293 +/- 1.8690, Acurácia: 0.2273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 140, Loss: 1.3897 +/- 1.1237, Acurácia: 0.5300\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 140, Loss: 2.8399 +/- 1.8891, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 141, Loss: 1.4279 +/- 1.1571, Acurácia: 0.5280\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 141, Loss: 2.8190 +/- 1.9338, Acurácia: 0.2273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 142, Loss: 1.3928 +/- 1.1316, Acurácia: 0.5282\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 142, Loss: 2.8833 +/- 2.0214, Acurácia: 0.2159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 143, Loss: 1.3981 +/- 1.1120, Acurácia: 0.5363\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 143, Loss: 2.7500 +/- 1.8570, Acurácia: 0.2273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 144, Loss: 1.4310 +/- 1.1332, Acurácia: 0.5246\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 144, Loss: 2.8358 +/- 1.9518, Acurácia: 0.2273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 145, Loss: 1.4423 +/- 1.1672, Acurácia: 0.5321\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 145, Loss: 2.6697 +/- 1.8479, Acurácia: 0.2386\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 146, Loss: 1.4258 +/- 1.0792, Acurácia: 0.5280\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 146, Loss: 2.7167 +/- 1.8371, Acurácia: 0.2500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 147, Loss: 1.4373 +/- 1.1162, Acurácia: 0.5084\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 147, Loss: 2.8465 +/- 1.9563, Acurácia: 0.2500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 148, Loss: 1.3407 +/- 1.0817, Acurácia: 0.5513\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 148, Loss: 2.8375 +/- 1.9364, Acurácia: 0.2500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 149, Loss: 1.4013 +/- 1.1250, Acurácia: 0.5327\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 149, Loss: 2.7044 +/- 1.8285, Acurácia: 0.2727\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 150, Loss: 1.4571 +/- 1.1400, Acurácia: 0.5244\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 150, Loss: 2.6736 +/- 1.8480, Acurácia: 0.2500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 151, Loss: 1.3652 +/- 1.0784, Acurácia: 0.5504\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 151, Loss: 2.5700 +/- 1.6647, Acurácia: 0.2727\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 152, Loss: 1.3837 +/- 1.0771, Acurácia: 0.5146\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 152, Loss: 2.6163 +/- 1.7697, Acurácia: 0.2614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 153, Loss: 1.3597 +/- 1.0998, Acurácia: 0.5548\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 153, Loss: 2.6061 +/- 1.7649, Acurácia: 0.2614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 154, Loss: 1.3551 +/- 1.0999, Acurácia: 0.5481\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 154, Loss: 2.5230 +/- 1.7478, Acurácia: 0.2955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 155, Loss: 1.4029 +/- 1.1048, Acurácia: 0.5338\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 155, Loss: 2.5585 +/- 1.9145, Acurácia: 0.2955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 156, Loss: 1.4237 +/- 1.1568, Acurácia: 0.5342\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 156, Loss: 2.5571 +/- 1.8887, Acurácia: 0.3068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 157, Loss: 1.3886 +/- 1.1055, Acurácia: 0.5440\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 157, Loss: 2.4663 +/- 1.9047, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 158, Loss: 1.3558 +/- 1.0850, Acurácia: 0.5595\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 158, Loss: 2.4875 +/- 1.9886, Acurácia: 0.2955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 159, Loss: 1.3726 +/- 1.0733, Acurácia: 0.5434\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 159, Loss: 2.4441 +/- 1.9666, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 160, Loss: 1.3749 +/- 1.0721, Acurácia: 0.5439\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 160, Loss: 2.5829 +/- 2.0703, Acurácia: 0.3523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 161, Loss: 1.3783 +/- 1.0976, Acurácia: 0.5416\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 161, Loss: 2.5105 +/- 1.9257, Acurácia: 0.3068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 162, Loss: 1.3759 +/- 1.0962, Acurácia: 0.5311\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 162, Loss: 2.5264 +/- 1.9021, Acurácia: 0.3068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 163, Loss: 1.3786 +/- 1.1130, Acurácia: 0.5457\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 163, Loss: 2.3592 +/- 1.6292, Acurácia: 0.3295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 164, Loss: 1.3897 +/- 1.1070, Acurácia: 0.5421\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 164, Loss: 2.3172 +/- 1.6385, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 165, Loss: 1.3443 +/- 1.0901, Acurácia: 0.5480\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 165, Loss: 2.3065 +/- 1.7134, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 166, Loss: 1.3339 +/- 1.1012, Acurácia: 0.5613\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 166, Loss: 2.3681 +/- 1.7996, Acurácia: 0.2955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 167, Loss: 1.3734 +/- 1.1424, Acurácia: 0.5568\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 167, Loss: 2.3498 +/- 1.7957, Acurácia: 0.3182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 168, Loss: 1.3682 +/- 1.1426, Acurácia: 0.5363\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 168, Loss: 2.1943 +/- 1.6567, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 169, Loss: 1.3855 +/- 1.1262, Acurácia: 0.5264\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 169, Loss: 2.2747 +/- 1.7980, Acurácia: 0.3523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 170, Loss: 1.3231 +/- 1.1177, Acurácia: 0.5825\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 170, Loss: 2.1683 +/- 1.6206, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 171, Loss: 1.3283 +/- 1.1137, Acurácia: 0.5690\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 171, Loss: 2.2133 +/- 1.7013, Acurácia: 0.3295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 172, Loss: 1.3294 +/- 1.1196, Acurácia: 0.5455\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 172, Loss: 2.3329 +/- 1.7846, Acurácia: 0.2841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 173, Loss: 1.3855 +/- 1.1132, Acurácia: 0.5360\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 173, Loss: 2.1943 +/- 1.6231, Acurácia: 0.3295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 174, Loss: 1.3140 +/- 1.0851, Acurácia: 0.5647\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 174, Loss: 2.2126 +/- 1.6208, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 175, Loss: 1.2781 +/- 1.0431, Acurácia: 0.5735\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 175, Loss: 2.2608 +/- 1.6922, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 176, Loss: 1.3634 +/- 1.1457, Acurácia: 0.5568\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 176, Loss: 2.2416 +/- 1.7124, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 177, Loss: 1.2590 +/- 1.0279, Acurácia: 0.5683\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 177, Loss: 2.1780 +/- 1.6591, Acurácia: 0.3523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 178, Loss: 1.3005 +/- 1.0952, Acurácia: 0.5672\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 178, Loss: 2.2383 +/- 1.7351, Acurácia: 0.3295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 179, Loss: 1.2757 +/- 1.0622, Acurácia: 0.5775\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 179, Loss: 2.1458 +/- 1.5327, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 180, Loss: 1.3166 +/- 1.1453, Acurácia: 0.5699\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 180, Loss: 2.1337 +/- 1.6321, Acurácia: 0.3636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 181, Loss: 1.2849 +/- 1.0968, Acurácia: 0.5799\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 181, Loss: 2.1183 +/- 1.5933, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 182, Loss: 1.2474 +/- 1.0678, Acurácia: 0.5932\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 182, Loss: 2.0662 +/- 1.5401, Acurácia: 0.3636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 183, Loss: 1.3133 +/- 1.0875, Acurácia: 0.5680\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 183, Loss: 2.0556 +/- 1.5196, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 184, Loss: 1.3461 +/- 1.0679, Acurácia: 0.5508\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 184, Loss: 2.0951 +/- 1.5579, Acurácia: 0.3750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 185, Loss: 1.3734 +/- 1.0936, Acurácia: 0.5498\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 185, Loss: 2.0471 +/- 1.5400, Acurácia: 0.3409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 186, Loss: 1.3213 +/- 1.0947, Acurácia: 0.5741\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 186, Loss: 2.0756 +/- 1.6164, Acurácia: 0.3977\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 187, Loss: 1.3106 +/- 1.0995, Acurácia: 0.5658\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 187, Loss: 2.0516 +/- 1.5278, Acurácia: 0.3864\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 188, Loss: 1.3631 +/- 1.1180, Acurácia: 0.5478\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 188, Loss: 1.9954 +/- 1.5876, Acurácia: 0.3523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 189, Loss: 1.3261 +/- 1.0509, Acurácia: 0.5568\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 189, Loss: 2.0105 +/- 1.5850, Acurácia: 0.3636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 190, Loss: 1.3078 +/- 1.1041, Acurácia: 0.5836\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 190, Loss: 2.0083 +/- 1.6548, Acurácia: 0.3636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 191, Loss: 1.3834 +/- 1.1021, Acurácia: 0.5498\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 191, Loss: 1.8681 +/- 1.4266, Acurácia: 0.3750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 192, Loss: 1.3104 +/- 1.0822, Acurácia: 0.5776\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 192, Loss: 1.9003 +/- 1.4694, Acurácia: 0.3523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 193, Loss: 1.2964 +/- 1.0520, Acurácia: 0.5718\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 193, Loss: 1.8666 +/- 1.4057, Acurácia: 0.3750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 194, Loss: 1.2871 +/- 1.1026, Acurácia: 0.5872\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 194, Loss: 1.8688 +/- 1.4236, Acurácia: 0.3750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 195, Loss: 1.3143 +/- 1.0956, Acurácia: 0.5778\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 195, Loss: 1.7996 +/- 1.3507, Acurácia: 0.3864\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 196, Loss: 1.3024 +/- 1.0977, Acurácia: 0.5855\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 196, Loss: 1.8074 +/- 1.4300, Acurácia: 0.3750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 197, Loss: 1.2724 +/- 1.0556, Acurácia: 0.5939\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 197, Loss: 1.8272 +/- 1.4599, Acurácia: 0.3864\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 198, Loss: 1.2622 +/- 1.0678, Acurácia: 0.5924\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 198, Loss: 1.7786 +/- 1.3868, Acurácia: 0.3636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 199, Loss: 1.2902 +/- 1.1079, Acurácia: 0.5761\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 199, Loss: 1.7631 +/- 1.4088, Acurácia: 0.3864\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 200, Loss: 1.2427 +/- 1.0551, Acurácia: 0.5970\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 200, Loss: 1.6446 +/- 1.2516, Acurácia: 0.3977\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 201, Loss: 1.3182 +/- 1.0982, Acurácia: 0.5755\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 201, Loss: 1.6447 +/- 1.2094, Acurácia: 0.4205\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 202, Loss: 1.3361 +/- 1.1028, Acurácia: 0.5809\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 202, Loss: 1.6672 +/- 1.2257, Acurácia: 0.4205\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 203, Loss: 1.2735 +/- 1.0991, Acurácia: 0.5950\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 203, Loss: 1.6586 +/- 1.2395, Acurácia: 0.3977\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 204, Loss: 1.2518 +/- 1.0516, Acurácia: 0.5884\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 204, Loss: 1.6428 +/- 1.2308, Acurácia: 0.3977\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 205, Loss: 1.2420 +/- 1.0192, Acurácia: 0.5921\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 205, Loss: 1.6220 +/- 1.2270, Acurácia: 0.4318\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 206, Loss: 1.2597 +/- 1.1267, Acurácia: 0.6076\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 206, Loss: 1.6263 +/- 1.1941, Acurácia: 0.4091\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 207, Loss: 1.3021 +/- 1.1320, Acurácia: 0.5872\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 207, Loss: 1.6189 +/- 1.1663, Acurácia: 0.3977\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 208, Loss: 1.2240 +/- 1.0894, Acurácia: 0.6254\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 208, Loss: 1.6224 +/- 1.1825, Acurácia: 0.3864\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 209, Loss: 1.2182 +/- 1.0550, Acurácia: 0.6299\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 209, Loss: 1.6731 +/- 1.3071, Acurácia: 0.4205\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 210, Loss: 1.2558 +/- 1.1046, Acurácia: 0.6116\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 210, Loss: 1.6532 +/- 1.2330, Acurácia: 0.4318\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 211, Loss: 1.1867 +/- 1.0754, Acurácia: 0.6256\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 211, Loss: 1.6159 +/- 1.2218, Acurácia: 0.4205\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 212, Loss: 1.1525 +/- 1.0951, Acurácia: 0.6375\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 212, Loss: 1.5843 +/- 1.2097, Acurácia: 0.4432\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 213, Loss: 1.1546 +/- 1.0789, Acurácia: 0.6376\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 213, Loss: 1.6002 +/- 1.2538, Acurácia: 0.4091\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 214, Loss: 1.1651 +/- 1.0628, Acurácia: 0.6276\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 214, Loss: 1.6032 +/- 1.2674, Acurácia: 0.4432\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 215, Loss: 1.1473 +/- 1.0512, Acurácia: 0.6287\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 215, Loss: 1.5928 +/- 1.2407, Acurácia: 0.4205\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 216, Loss: 1.1572 +/- 1.0848, Acurácia: 0.6291\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 216, Loss: 1.5544 +/- 1.2595, Acurácia: 0.4545\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 217, Loss: 1.2154 +/- 1.0996, Acurácia: 0.6085\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 217, Loss: 1.5363 +/- 1.2154, Acurácia: 0.4659\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 218, Loss: 1.1688 +/- 1.1032, Acurácia: 0.6299\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 218, Loss: 1.5379 +/- 1.2673, Acurácia: 0.4773\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 219, Loss: 1.1117 +/- 1.0603, Acurácia: 0.6478\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 219, Loss: 1.5202 +/- 1.2052, Acurácia: 0.4432\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 220, Loss: 1.1169 +/- 1.0575, Acurácia: 0.6475\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 220, Loss: 1.5083 +/- 1.2215, Acurácia: 0.4545\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 221, Loss: 1.1171 +/- 1.0372, Acurácia: 0.6403\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 221, Loss: 1.4679 +/- 1.1948, Acurácia: 0.4659\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 222, Loss: 1.1168 +/- 1.0676, Acurácia: 0.6384\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 222, Loss: 1.4728 +/- 1.1713, Acurácia: 0.4659\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 223, Loss: 1.0960 +/- 1.0446, Acurácia: 0.6480\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 223, Loss: 1.3982 +/- 1.1373, Acurácia: 0.4545\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 224, Loss: 1.1259 +/- 1.0918, Acurácia: 0.6429\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 224, Loss: 1.4139 +/- 1.1908, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 225, Loss: 1.1097 +/- 1.0518, Acurácia: 0.6449\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 225, Loss: 1.3886 +/- 1.1874, Acurácia: 0.5227\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 226, Loss: 1.0422 +/- 0.9781, Acurácia: 0.6626\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 226, Loss: 1.3771 +/- 1.1414, Acurácia: 0.5227\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 227, Loss: 1.1063 +/- 1.0976, Acurácia: 0.6534\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 227, Loss: 1.4137 +/- 1.1853, Acurácia: 0.5000\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 228, Loss: 1.0569 +/- 1.0007, Acurácia: 0.6467\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 228, Loss: 1.3953 +/- 1.2085, Acurácia: 0.4886\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 229, Loss: 1.0968 +/- 1.0661, Acurácia: 0.6419\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 229, Loss: 1.3436 +/- 1.1638, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 230, Loss: 1.0801 +/- 1.0476, Acurácia: 0.6579\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 230, Loss: 1.3382 +/- 1.1884, Acurácia: 0.5000\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 231, Loss: 1.0764 +/- 1.0511, Acurácia: 0.6524\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 231, Loss: 1.3481 +/- 1.2427, Acurácia: 0.5455\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 232, Loss: 1.0357 +/- 1.0260, Acurácia: 0.6703\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 232, Loss: 1.3487 +/- 1.1843, Acurácia: 0.5227\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 233, Loss: 1.0795 +/- 1.0672, Acurácia: 0.6676\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 233, Loss: 1.3687 +/- 1.2579, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 234, Loss: 1.0917 +/- 1.0627, Acurácia: 0.6545\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 234, Loss: 1.3682 +/- 1.2347, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 235, Loss: 1.0476 +/- 1.0488, Acurácia: 0.6699\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 235, Loss: 1.3728 +/- 1.2724, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 236, Loss: 1.0501 +/- 1.0681, Acurácia: 0.6813\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 236, Loss: 1.3494 +/- 1.1789, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 237, Loss: 1.0423 +/- 1.0804, Acurácia: 0.6731\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 237, Loss: 1.3114 +/- 1.1391, Acurácia: 0.5114\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 238, Loss: 1.0305 +/- 1.0453, Acurácia: 0.6693\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 238, Loss: 1.3647 +/- 1.2113, Acurácia: 0.5000\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 239, Loss: 0.9951 +/- 1.0342, Acurácia: 0.6916\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 239, Loss: 1.3464 +/- 1.2160, Acurácia: 0.5455\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 240, Loss: 1.0202 +/- 1.0480, Acurácia: 0.6726\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 240, Loss: 1.3375 +/- 1.1922, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 241, Loss: 0.9591 +/- 0.9918, Acurácia: 0.6978\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 241, Loss: 1.3167 +/- 1.2134, Acurácia: 0.5227\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 242, Loss: 0.9897 +/- 1.0583, Acurácia: 0.6852\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 242, Loss: 1.2902 +/- 1.1564, Acurácia: 0.5227\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 243, Loss: 0.9720 +/- 1.0755, Acurácia: 0.7143\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 243, Loss: 1.2959 +/- 1.1640, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 244, Loss: 0.9973 +/- 1.0329, Acurácia: 0.6703\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 244, Loss: 1.2486 +/- 1.1026, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 245, Loss: 0.9755 +/- 1.0522, Acurácia: 0.6874\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 245, Loss: 1.2590 +/- 1.1249, Acurácia: 0.5568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 246, Loss: 0.9823 +/- 0.9957, Acurácia: 0.6882\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 246, Loss: 1.2439 +/- 1.1218, Acurácia: 0.5795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 247, Loss: 0.9820 +/- 1.0550, Acurácia: 0.7026\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 247, Loss: 1.1907 +/- 1.0392, Acurácia: 0.5795\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 248, Loss: 0.9411 +/- 1.0022, Acurácia: 0.6970\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 248, Loss: 1.1849 +/- 1.0715, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 249, Loss: 0.9694 +/- 1.0290, Acurácia: 0.6988\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 249, Loss: 1.2249 +/- 1.1071, Acurácia: 0.6023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 250, Loss: 0.9674 +/- 1.0067, Acurácia: 0.6813\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 250, Loss: 1.2037 +/- 1.1019, Acurácia: 0.5909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 251, Loss: 0.9208 +/- 0.9820, Acurácia: 0.6923\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 251, Loss: 1.1908 +/- 1.0822, Acurácia: 0.5455\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 252, Loss: 0.9695 +/- 1.0721, Acurácia: 0.7050\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 252, Loss: 1.2343 +/- 1.1173, Acurácia: 0.5227\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 253, Loss: 0.9371 +/- 0.9933, Acurácia: 0.7092\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 253, Loss: 1.2066 +/- 1.0770, Acurácia: 0.5455\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 254, Loss: 0.9274 +/- 0.9971, Acurácia: 0.7052\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 254, Loss: 1.2063 +/- 1.0947, Acurácia: 0.5455\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 255, Loss: 0.9443 +/- 1.0071, Acurácia: 0.6928\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 255, Loss: 1.2101 +/- 1.1183, Acurácia: 0.5568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 256, Loss: 0.8922 +/- 0.9728, Acurácia: 0.7257\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 256, Loss: 1.2211 +/- 1.1235, Acurácia: 0.5341\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 257, Loss: 0.9403 +/- 1.0195, Acurácia: 0.7110\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 257, Loss: 1.2241 +/- 1.1390, Acurácia: 0.5227\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 258, Loss: 0.9022 +/- 0.9810, Acurácia: 0.7159\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 258, Loss: 1.1752 +/- 1.0878, Acurácia: 0.5455\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 259, Loss: 0.9343 +/- 1.0340, Acurácia: 0.6931\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 259, Loss: 1.2157 +/- 1.1438, Acurácia: 0.5568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 260, Loss: 0.8968 +/- 0.9927, Acurácia: 0.7174\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 260, Loss: 1.1817 +/- 1.0999, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 261, Loss: 0.8661 +/- 0.9650, Acurácia: 0.7323\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 261, Loss: 1.1845 +/- 1.1040, Acurácia: 0.5568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 262, Loss: 0.8725 +/- 0.9923, Acurácia: 0.7242\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 262, Loss: 1.2054 +/- 1.1457, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 263, Loss: 0.8756 +/- 0.9843, Acurácia: 0.7200\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 263, Loss: 1.1999 +/- 1.1521, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 264, Loss: 0.9143 +/- 1.0139, Acurácia: 0.7069\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 264, Loss: 1.1699 +/- 1.0938, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 265, Loss: 0.9159 +/- 1.0174, Acurácia: 0.7120\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 265, Loss: 1.1906 +/- 1.0953, Acurácia: 0.5568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 266, Loss: 0.9154 +/- 0.9849, Acurácia: 0.7015\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 266, Loss: 1.1531 +/- 1.0843, Acurácia: 0.5568\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 267, Loss: 0.8118 +/- 0.9480, Acurácia: 0.7522\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 267, Loss: 1.1139 +/- 1.1061, Acurácia: 0.6023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 268, Loss: 0.8691 +/- 1.0134, Acurácia: 0.7234\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 268, Loss: 1.1248 +/- 1.1112, Acurácia: 0.5682\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 269, Loss: 0.8743 +/- 0.9710, Acurácia: 0.7257\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 269, Loss: 1.1290 +/- 1.1422, Acurácia: 0.5909\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 270, Loss: 0.8705 +/- 0.9995, Acurácia: 0.7329\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 270, Loss: 1.0763 +/- 1.1067, Acurácia: 0.6477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 271, Loss: 0.8627 +/- 0.9842, Acurácia: 0.7259\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 271, Loss: 1.0913 +/- 1.1367, Acurácia: 0.6023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 272, Loss: 0.8838 +/- 1.0230, Acurácia: 0.7247\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 272, Loss: 1.0671 +/- 1.1193, Acurácia: 0.6136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 273, Loss: 0.8513 +/- 0.9941, Acurácia: 0.7367\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 273, Loss: 1.0845 +/- 1.0563, Acurácia: 0.6023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 274, Loss: 0.8630 +/- 0.9775, Acurácia: 0.7319\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 274, Loss: 1.0432 +/- 1.0489, Acurácia: 0.6364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 275, Loss: 0.8289 +/- 0.9658, Acurácia: 0.7485\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 275, Loss: 1.0178 +/- 1.0434, Acurácia: 0.6591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 276, Loss: 0.8561 +/- 0.9916, Acurácia: 0.7252\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 276, Loss: 1.0538 +/- 1.0603, Acurácia: 0.6023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 277, Loss: 0.7863 +/- 0.9092, Acurácia: 0.7532\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 277, Loss: 1.0435 +/- 1.1023, Acurácia: 0.6136\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 278, Loss: 0.8473 +/- 0.9850, Acurácia: 0.7476\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 278, Loss: 1.0133 +/- 1.0501, Acurácia: 0.6023\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 279, Loss: 0.7539 +/- 0.8668, Acurácia: 0.7762\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 279, Loss: 1.0005 +/- 1.0627, Acurácia: 0.6591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 280, Loss: 0.7731 +/- 0.8795, Acurácia: 0.7665\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 280, Loss: 0.9619 +/- 1.0120, Acurácia: 0.6591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 281, Loss: 0.8561 +/- 1.0258, Acurácia: 0.7305\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 281, Loss: 1.0189 +/- 1.0714, Acurácia: 0.6364\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 282, Loss: 0.8342 +/- 1.0163, Acurácia: 0.7335\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 282, Loss: 0.9801 +/- 1.0068, Acurácia: 0.6591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 283, Loss: 0.7963 +/- 0.9509, Acurácia: 0.7424\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 283, Loss: 0.9881 +/- 1.0521, Acurácia: 0.6932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 284, Loss: 0.8048 +/- 0.9684, Acurácia: 0.7493\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 284, Loss: 0.9611 +/- 0.9986, Acurácia: 0.6591\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 285, Loss: 0.8149 +/- 0.9902, Acurácia: 0.7388\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 285, Loss: 1.0055 +/- 1.0147, Acurácia: 0.6477\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 286, Loss: 0.8312 +/- 1.0047, Acurácia: 0.7430\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 286, Loss: 0.9183 +/- 1.0106, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 287, Loss: 0.7274 +/- 0.9030, Acurácia: 0.7820\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 287, Loss: 0.9467 +/- 1.0036, Acurácia: 0.6705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 288, Loss: 0.7336 +/- 0.8951, Acurácia: 0.7727\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 288, Loss: 0.9445 +/- 1.0101, Acurácia: 0.6705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 289, Loss: 0.7834 +/- 0.9760, Acurácia: 0.7402\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 289, Loss: 0.9669 +/- 1.0457, Acurácia: 0.6705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 290, Loss: 0.8117 +/- 0.9977, Acurácia: 0.7428\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 290, Loss: 0.9223 +/- 0.9886, Acurácia: 0.6818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 291, Loss: 0.7772 +/- 0.9762, Acurácia: 0.7641\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 291, Loss: 0.9375 +/- 0.9828, Acurácia: 0.6932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 292, Loss: 0.7729 +/- 0.9680, Acurácia: 0.7638\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 292, Loss: 0.9006 +/- 0.9752, Acurácia: 0.6932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 293, Loss: 0.7536 +/- 0.9487, Acurácia: 0.7632\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 293, Loss: 0.9028 +/- 0.9676, Acurácia: 0.7045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 294, Loss: 0.7167 +/- 0.9013, Acurácia: 0.7773\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 294, Loss: 0.8889 +/- 0.9591, Acurácia: 0.6818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 295, Loss: 0.7345 +/- 0.8840, Acurácia: 0.7617\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 295, Loss: 0.9053 +/- 0.9648, Acurácia: 0.6705\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 296, Loss: 0.7689 +/- 0.9583, Acurácia: 0.7628\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 296, Loss: 0.8943 +/- 0.9639, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 297, Loss: 0.7400 +/- 0.9318, Acurácia: 0.7689\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 297, Loss: 0.8738 +/- 0.9736, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 298, Loss: 0.7185 +/- 0.8783, Acurácia: 0.7662\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 298, Loss: 0.8794 +/- 0.9734, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 299, Loss: 0.7047 +/- 0.9076, Acurácia: 0.7810\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 299, Loss: 0.9040 +/- 1.0468, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 300, Loss: 0.7350 +/- 0.9958, Acurácia: 0.7814\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 300, Loss: 0.8627 +/- 0.9212, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 301, Loss: 0.7191 +/- 0.8997, Acurácia: 0.7769\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 301, Loss: 0.8795 +/- 0.9955, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 302, Loss: 0.7272 +/- 0.9358, Acurácia: 0.7775\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 302, Loss: 0.9233 +/- 1.0034, Acurácia: 0.6932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 303, Loss: 0.7290 +/- 0.9633, Acurácia: 0.7768\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 303, Loss: 0.8679 +/- 0.9405, Acurácia: 0.6932\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 304, Loss: 0.7066 +/- 0.8970, Acurácia: 0.7847\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 304, Loss: 0.8447 +/- 0.9383, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 305, Loss: 0.6822 +/- 0.8681, Acurácia: 0.7771\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 305, Loss: 0.8325 +/- 0.9472, Acurácia: 0.7045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 306, Loss: 0.7308 +/- 0.9611, Acurácia: 0.7718\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 306, Loss: 0.8133 +/- 0.9168, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 307, Loss: 0.6978 +/- 0.9586, Acurácia: 0.7783\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 307, Loss: 0.8187 +/- 0.9309, Acurácia: 0.6818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 308, Loss: 0.7411 +/- 1.0302, Acurácia: 0.7728\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 308, Loss: 0.8747 +/- 0.9751, Acurácia: 0.7045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 309, Loss: 0.7044 +/- 0.9260, Acurácia: 0.7854\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 309, Loss: 0.8592 +/- 0.9899, Acurácia: 0.7045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 310, Loss: 0.6926 +/- 0.8841, Acurácia: 0.7636\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 310, Loss: 0.8700 +/- 0.9960, Acurácia: 0.6818\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 311, Loss: 0.6807 +/- 0.8664, Acurácia: 0.7865\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 311, Loss: 0.8338 +/- 0.9644, Acurácia: 0.7386\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 312, Loss: 0.6520 +/- 0.8302, Acurácia: 0.8030\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 312, Loss: 0.8143 +/- 0.9560, Acurácia: 0.7386\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 313, Loss: 0.6607 +/- 0.8648, Acurácia: 0.8044\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 313, Loss: 0.7985 +/- 0.9283, Acurácia: 0.7045\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 314, Loss: 0.6476 +/- 0.9214, Acurácia: 0.8187\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 314, Loss: 0.7735 +/- 0.8863, Acurácia: 0.7500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 315, Loss: 0.6987 +/- 0.9206, Acurácia: 0.7844\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 315, Loss: 0.7780 +/- 0.8964, Acurácia: 0.7500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 316, Loss: 0.7003 +/- 0.9767, Acurácia: 0.7930\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 316, Loss: 0.7726 +/- 0.8865, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 317, Loss: 0.6668 +/- 0.9430, Acurácia: 0.8042\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 317, Loss: 0.7896 +/- 0.8994, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 318, Loss: 0.6471 +/- 0.9157, Acurácia: 0.8204\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 318, Loss: 0.7827 +/- 0.8971, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 319, Loss: 0.6472 +/- 0.8352, Acurácia: 0.7908\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 319, Loss: 0.7794 +/- 0.9197, Acurácia: 0.7386\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 320, Loss: 0.6781 +/- 0.9647, Acurácia: 0.7996\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 320, Loss: 0.7178 +/- 0.8443, Acurácia: 0.7500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 321, Loss: 0.6437 +/- 0.8635, Acurácia: 0.8034\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 321, Loss: 0.7761 +/- 0.8742, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 322, Loss: 0.6537 +/- 0.8714, Acurácia: 0.7882\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 322, Loss: 0.7456 +/- 0.8366, Acurácia: 0.7386\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 323, Loss: 0.6517 +/- 0.8849, Acurácia: 0.8012\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 323, Loss: 0.7571 +/- 0.8821, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 324, Loss: 0.6658 +/- 0.9623, Acurácia: 0.8030\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 324, Loss: 0.7567 +/- 0.8778, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 325, Loss: 0.6685 +/- 0.9090, Acurácia: 0.7880\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 325, Loss: 0.7579 +/- 0.8932, Acurácia: 0.7500\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 326, Loss: 0.6198 +/- 0.8461, Acurácia: 0.8154\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 326, Loss: 0.7756 +/- 0.9224, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 327, Loss: 0.6587 +/- 0.9117, Acurácia: 0.8039\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 327, Loss: 0.7331 +/- 0.8845, Acurácia: 0.7614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 328, Loss: 0.6428 +/- 0.8777, Acurácia: 0.8105\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 328, Loss: 0.7199 +/- 0.8699, Acurácia: 0.7386\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 329, Loss: 0.6453 +/- 0.8548, Acurácia: 0.8071\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 329, Loss: 0.7345 +/- 0.8556, Acurácia: 0.7273\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 330, Loss: 0.6268 +/- 0.9117, Acurácia: 0.8215\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 330, Loss: 0.7196 +/- 0.8535, Acurácia: 0.7614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 331, Loss: 0.6289 +/- 0.9431, Acurácia: 0.8201\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 331, Loss: 0.7412 +/- 0.8707, Acurácia: 0.7159\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 332, Loss: 0.6222 +/- 0.8855, Acurácia: 0.7975\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 332, Loss: 0.6700 +/- 0.8128, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 333, Loss: 0.6039 +/- 0.8395, Acurácia: 0.8037\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 333, Loss: 0.6811 +/- 0.8490, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 334, Loss: 0.6045 +/- 0.8716, Acurácia: 0.8123\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 334, Loss: 0.6325 +/- 0.7975, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 335, Loss: 0.6175 +/- 0.8680, Acurácia: 0.8147\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 335, Loss: 0.6608 +/- 0.8137, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 336, Loss: 0.6136 +/- 0.8221, Acurácia: 0.8201\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 336, Loss: 0.6504 +/- 0.7911, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 337, Loss: 0.6033 +/- 0.8125, Acurácia: 0.8211\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 337, Loss: 0.6491 +/- 0.8067, Acurácia: 0.7614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 338, Loss: 0.5822 +/- 0.8260, Acurácia: 0.8215\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 338, Loss: 0.6399 +/- 0.8110, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 339, Loss: 0.5886 +/- 0.8402, Acurácia: 0.8228\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 339, Loss: 0.6741 +/- 0.8460, Acurácia: 0.7614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 340, Loss: 0.6113 +/- 0.8519, Acurácia: 0.8192\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 340, Loss: 0.6472 +/- 0.8000, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 341, Loss: 0.6184 +/- 0.9107, Acurácia: 0.8177\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 341, Loss: 0.6128 +/- 0.7864, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 342, Loss: 0.6027 +/- 0.8453, Acurácia: 0.8275\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 342, Loss: 0.6366 +/- 0.8036, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 343, Loss: 0.5706 +/- 0.8124, Acurácia: 0.8158\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 343, Loss: 0.6346 +/- 0.8068, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 344, Loss: 0.6066 +/- 0.8797, Acurácia: 0.8165\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 344, Loss: 0.6433 +/- 0.7991, Acurácia: 0.7614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 345, Loss: 0.5524 +/- 0.8522, Acurácia: 0.8476\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 345, Loss: 0.6336 +/- 0.8060, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 346, Loss: 0.5800 +/- 0.8461, Acurácia: 0.8329\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 346, Loss: 0.6263 +/- 0.7992, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 347, Loss: 0.5692 +/- 0.8383, Acurácia: 0.8394\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 347, Loss: 0.6233 +/- 0.7920, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 348, Loss: 0.5649 +/- 0.8677, Acurácia: 0.8453\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 348, Loss: 0.6865 +/- 0.9460, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 349, Loss: 0.5485 +/- 0.8793, Acurácia: 0.8485\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 349, Loss: 0.6113 +/- 0.7851, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 350, Loss: 0.5733 +/- 0.8917, Acurácia: 0.8338\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 350, Loss: 0.6397 +/- 0.8562, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 351, Loss: 0.5951 +/- 0.9113, Acurácia: 0.8275\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 351, Loss: 0.6292 +/- 0.8363, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 352, Loss: 0.5701 +/- 0.8469, Acurácia: 0.8301\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 352, Loss: 0.6331 +/- 0.8486, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 353, Loss: 0.5350 +/- 0.7830, Acurácia: 0.8425\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 353, Loss: 0.6148 +/- 0.8344, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 354, Loss: 0.5895 +/- 0.9056, Acurácia: 0.8344\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 354, Loss: 0.6350 +/- 0.8202, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 355, Loss: 0.5581 +/- 0.8832, Acurácia: 0.8429\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 355, Loss: 0.6058 +/- 0.8320, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 356, Loss: 0.5910 +/- 0.8855, Acurácia: 0.8235\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 356, Loss: 0.6029 +/- 0.8113, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 357, Loss: 0.5668 +/- 0.8248, Acurácia: 0.8325\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 357, Loss: 0.6175 +/- 0.8387, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 358, Loss: 0.5615 +/- 0.8598, Acurácia: 0.8408\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 358, Loss: 0.6097 +/- 0.8432, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 359, Loss: 0.5152 +/- 0.8159, Acurácia: 0.8551\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 359, Loss: 0.6109 +/- 0.8299, Acurácia: 0.7841\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 360, Loss: 0.5317 +/- 0.7806, Acurácia: 0.8434\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 360, Loss: 0.5990 +/- 0.8291, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 361, Loss: 0.5521 +/- 0.8937, Acurácia: 0.8466\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 361, Loss: 0.6021 +/- 0.8391, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 362, Loss: 0.5837 +/- 0.9074, Acurácia: 0.8295\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 362, Loss: 0.6383 +/- 0.8473, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 363, Loss: 0.5430 +/- 0.8137, Acurácia: 0.8389\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 363, Loss: 0.5967 +/- 0.8310, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 364, Loss: 0.5687 +/- 0.8991, Acurácia: 0.8275\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 364, Loss: 0.6034 +/- 0.8515, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 365, Loss: 0.5369 +/- 0.8478, Acurácia: 0.8422\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 365, Loss: 0.6102 +/- 0.8432, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 366, Loss: 0.5420 +/- 0.8130, Acurácia: 0.8478\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 366, Loss: 0.6048 +/- 0.8283, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 367, Loss: 0.5395 +/- 0.8069, Acurácia: 0.8431\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 367, Loss: 0.5939 +/- 0.7984, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 368, Loss: 0.5246 +/- 0.8788, Acurácia: 0.8491\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 368, Loss: 0.5666 +/- 0.7959, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 369, Loss: 0.5283 +/- 0.8464, Acurácia: 0.8473\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 369, Loss: 0.5813 +/- 0.8049, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 370, Loss: 0.5832 +/- 0.8834, Acurácia: 0.8238\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 370, Loss: 0.5560 +/- 0.7679, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 371, Loss: 0.5066 +/- 0.7793, Acurácia: 0.8436\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 371, Loss: 0.5492 +/- 0.7738, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 372, Loss: 0.4873 +/- 0.7475, Acurácia: 0.8530\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 372, Loss: 0.5783 +/- 0.7875, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 373, Loss: 0.4822 +/- 0.7839, Acurácia: 0.8598\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 373, Loss: 0.5588 +/- 0.8072, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 374, Loss: 0.5458 +/- 0.8730, Acurácia: 0.8402\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 374, Loss: 0.5577 +/- 0.7852, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 375, Loss: 0.4942 +/- 0.7825, Acurácia: 0.8450\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 375, Loss: 0.5858 +/- 0.8036, Acurácia: 0.7955\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 376, Loss: 0.5148 +/- 0.8410, Acurácia: 0.8497\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 376, Loss: 0.5895 +/- 0.7898, Acurácia: 0.7614\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 377, Loss: 0.5097 +/- 0.8298, Acurácia: 0.8499\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 377, Loss: 0.5633 +/- 0.7869, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 378, Loss: 0.5088 +/- 0.8829, Acurácia: 0.8568\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 378, Loss: 0.5636 +/- 0.7838, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 379, Loss: 0.4530 +/- 0.7105, Acurácia: 0.8706\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 379, Loss: 0.5697 +/- 0.7974, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 380, Loss: 0.5018 +/- 0.8609, Acurácia: 0.8566\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 380, Loss: 0.6040 +/- 0.8295, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 381, Loss: 0.5166 +/- 0.8481, Acurácia: 0.8404\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 381, Loss: 0.5553 +/- 0.7990, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 382, Loss: 0.4980 +/- 0.7837, Acurácia: 0.8491\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 382, Loss: 0.5693 +/- 0.7910, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 383, Loss: 0.4719 +/- 0.7580, Acurácia: 0.8550\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 383, Loss: 0.5489 +/- 0.7819, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 384, Loss: 0.5001 +/- 0.8117, Acurácia: 0.8495\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 384, Loss: 0.5433 +/- 0.7804, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 385, Loss: 0.4539 +/- 0.7642, Acurácia: 0.8635\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 385, Loss: 0.5460 +/- 0.7996, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 386, Loss: 0.4774 +/- 0.7369, Acurácia: 0.8502\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 386, Loss: 0.5585 +/- 0.7835, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 387, Loss: 0.4952 +/- 0.8565, Acurácia: 0.8496\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 387, Loss: 0.5553 +/- 0.8251, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 388, Loss: 0.4532 +/- 0.7309, Acurácia: 0.8617\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 388, Loss: 0.5554 +/- 0.7993, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 389, Loss: 0.4648 +/- 0.7832, Acurácia: 0.8602\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 389, Loss: 0.5676 +/- 0.8138, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 390, Loss: 0.4547 +/- 0.7556, Acurácia: 0.8650\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 390, Loss: 0.5379 +/- 0.8137, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 391, Loss: 0.4591 +/- 0.7989, Acurácia: 0.8602\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 391, Loss: 0.5427 +/- 0.8000, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 392, Loss: 0.4728 +/- 0.7494, Acurácia: 0.8570\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 392, Loss: 0.5455 +/- 0.8063, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 393, Loss: 0.4804 +/- 0.8543, Acurácia: 0.8639\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 393, Loss: 0.5568 +/- 0.7979, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 394, Loss: 0.4848 +/- 0.8093, Acurácia: 0.8566\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 394, Loss: 0.5275 +/- 0.7972, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 395, Loss: 0.4688 +/- 0.8357, Acurácia: 0.8621\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 395, Loss: 0.5379 +/- 0.8134, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 396, Loss: 0.4704 +/- 0.7937, Acurácia: 0.8656\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 396, Loss: 0.5162 +/- 0.7689, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 397, Loss: 0.4591 +/- 0.8096, Acurácia: 0.8635\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 397, Loss: 0.5258 +/- 0.7816, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 398, Loss: 0.4488 +/- 0.7364, Acurácia: 0.8589\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 398, Loss: 0.5239 +/- 0.7936, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 399, Loss: 0.4259 +/- 0.7461, Acurácia: 0.8743\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 399, Loss: 0.5183 +/- 0.7949, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 400, Loss: 0.4471 +/- 0.8322, Acurácia: 0.8640\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 400, Loss: 0.5076 +/- 0.7869, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 401, Loss: 0.4720 +/- 0.8296, Acurácia: 0.8750\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 401, Loss: 0.5296 +/- 0.8200, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 402, Loss: 0.4897 +/- 0.8484, Acurácia: 0.8583\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 402, Loss: 0.5264 +/- 0.7710, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 403, Loss: 0.4339 +/- 0.7163, Acurácia: 0.8693\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 403, Loss: 0.5105 +/- 0.7672, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 404, Loss: 0.4621 +/- 0.7889, Acurácia: 0.8628\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 404, Loss: 0.5163 +/- 0.8250, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 405, Loss: 0.4624 +/- 0.8044, Acurácia: 0.8515\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 405, Loss: 0.5111 +/- 0.8026, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 406, Loss: 0.4130 +/- 0.6976, Acurácia: 0.8718\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 406, Loss: 0.5268 +/- 0.8184, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 407, Loss: 0.4775 +/- 0.8704, Acurácia: 0.8585\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 407, Loss: 0.4950 +/- 0.7612, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 408, Loss: 0.4476 +/- 0.8205, Acurácia: 0.8712\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 408, Loss: 0.5032 +/- 0.7927, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 409, Loss: 0.4361 +/- 0.7545, Acurácia: 0.8759\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 409, Loss: 0.5098 +/- 0.7697, Acurácia: 0.8182\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 410, Loss: 0.4509 +/- 0.7728, Acurácia: 0.8681\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 410, Loss: 0.4954 +/- 0.7737, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 411, Loss: 0.4568 +/- 0.8223, Acurácia: 0.8694\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 411, Loss: 0.4812 +/- 0.7486, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 412, Loss: 0.4329 +/- 0.7044, Acurácia: 0.8769\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 412, Loss: 0.5139 +/- 0.7935, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 413, Loss: 0.4305 +/- 0.7935, Acurácia: 0.8889\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 413, Loss: 0.4868 +/- 0.7534, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 414, Loss: 0.4246 +/- 0.7446, Acurácia: 0.8796\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 414, Loss: 0.4765 +/- 0.7411, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 415, Loss: 0.4470 +/- 0.8167, Acurácia: 0.8750\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 415, Loss: 0.4934 +/- 0.7804, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 416, Loss: 0.4160 +/- 0.7392, Acurácia: 0.8795\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 416, Loss: 0.4989 +/- 0.8057, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 417, Loss: 0.4021 +/- 0.7414, Acurácia: 0.8856\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 417, Loss: 0.4681 +/- 0.7764, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 418, Loss: 0.3996 +/- 0.7578, Acurácia: 0.8899\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 418, Loss: 0.4789 +/- 0.7966, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 419, Loss: 0.4238 +/- 0.7473, Acurácia: 0.8673\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 419, Loss: 0.4842 +/- 0.7880, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 420, Loss: 0.4031 +/- 0.6967, Acurácia: 0.8772\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 420, Loss: 0.4877 +/- 0.8134, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 421, Loss: 0.4127 +/- 0.7515, Acurácia: 0.8781\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 421, Loss: 0.5086 +/- 0.8441, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 422, Loss: 0.3970 +/- 0.7497, Acurácia: 0.8850\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 422, Loss: 0.5026 +/- 0.8079, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 423, Loss: 0.4070 +/- 0.7698, Acurácia: 0.8791\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 423, Loss: 0.4851 +/- 0.8215, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 424, Loss: 0.4140 +/- 0.7508, Acurácia: 0.8739\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 424, Loss: 0.4809 +/- 0.8204, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 425, Loss: 0.4314 +/- 0.8226, Acurácia: 0.8822\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 425, Loss: 0.4689 +/- 0.7903, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 426, Loss: 0.3847 +/- 0.6807, Acurácia: 0.8837\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 426, Loss: 0.4695 +/- 0.7722, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 427, Loss: 0.4128 +/- 0.7168, Acurácia: 0.8760\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 427, Loss: 0.4794 +/- 0.8026, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 428, Loss: 0.3898 +/- 0.7388, Acurácia: 0.8916\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 428, Loss: 0.4777 +/- 0.8034, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 429, Loss: 0.4219 +/- 0.7944, Acurácia: 0.8691\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 429, Loss: 0.4675 +/- 0.7849, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 430, Loss: 0.3939 +/- 0.7657, Acurácia: 0.8905\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 430, Loss: 0.4646 +/- 0.7921, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 431, Loss: 0.4054 +/- 0.8100, Acurácia: 0.8841\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 431, Loss: 0.4632 +/- 0.7545, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 432, Loss: 0.3707 +/- 0.6618, Acurácia: 0.8994\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 432, Loss: 0.4795 +/- 0.7799, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 433, Loss: 0.4065 +/- 0.7323, Acurácia: 0.8750\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 433, Loss: 0.4708 +/- 0.7481, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 434, Loss: 0.4043 +/- 0.7640, Acurácia: 0.8909\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 434, Loss: 0.4885 +/- 0.7753, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 435, Loss: 0.3957 +/- 0.7408, Acurácia: 0.8865\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 435, Loss: 0.4733 +/- 0.7585, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 436, Loss: 0.3802 +/- 0.7152, Acurácia: 0.8822\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 436, Loss: 0.5025 +/- 0.7994, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 437, Loss: 0.4031 +/- 0.7107, Acurácia: 0.8792\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 437, Loss: 0.4972 +/- 0.7855, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 438, Loss: 0.4017 +/- 0.7293, Acurácia: 0.8784\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 438, Loss: 0.4846 +/- 0.7504, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 439, Loss: 0.4001 +/- 0.8020, Acurácia: 0.8794\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 439, Loss: 0.4897 +/- 0.7849, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 440, Loss: 0.3955 +/- 0.7491, Acurácia: 0.8791\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 440, Loss: 0.4551 +/- 0.7775, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 441, Loss: 0.3970 +/- 0.7619, Acurácia: 0.8796\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 441, Loss: 0.4741 +/- 0.7615, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 442, Loss: 0.3671 +/- 0.6976, Acurácia: 0.8878\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 442, Loss: 0.4726 +/- 0.7834, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 443, Loss: 0.4221 +/- 0.8425, Acurácia: 0.8766\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 443, Loss: 0.4635 +/- 0.7784, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 444, Loss: 0.3998 +/- 0.7799, Acurácia: 0.8711\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 444, Loss: 0.4551 +/- 0.7514, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 445, Loss: 0.4108 +/- 0.7971, Acurácia: 0.8788\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 445, Loss: 0.4757 +/- 0.7576, Acurácia: 0.8068\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 446, Loss: 0.3836 +/- 0.7113, Acurácia: 0.8914\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 446, Loss: 0.4748 +/- 0.7970, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 447, Loss: 0.3979 +/- 0.7435, Acurácia: 0.8866\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 447, Loss: 0.4337 +/- 0.7370, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 448, Loss: 0.3791 +/- 0.7283, Acurácia: 0.8900\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 448, Loss: 0.4429 +/- 0.7448, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 449, Loss: 0.3895 +/- 0.7583, Acurácia: 0.8815\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 449, Loss: 0.4290 +/- 0.7223, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 450, Loss: 0.3618 +/- 0.6850, Acurácia: 0.8905\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 450, Loss: 0.4484 +/- 0.7485, Acurácia: 0.8523\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 451, Loss: 0.3774 +/- 0.7503, Acurácia: 0.8826\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 451, Loss: 0.4359 +/- 0.7381, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 452, Loss: 0.3393 +/- 0.6361, Acurácia: 0.8980\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 452, Loss: 0.4417 +/- 0.7334, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 453, Loss: 0.3801 +/- 0.7675, Acurácia: 0.8931\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 453, Loss: 0.4521 +/- 0.7440, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 454, Loss: 0.3672 +/- 0.7056, Acurácia: 0.8910\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 454, Loss: 0.4586 +/- 0.7765, Acurácia: 0.8295\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 455, Loss: 0.3507 +/- 0.6553, Acurácia: 0.8925\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 455, Loss: 0.4584 +/- 0.8209, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 456, Loss: 0.3392 +/- 0.6481, Acurácia: 0.8989\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 456, Loss: 0.4684 +/- 0.8168, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 457, Loss: 0.3826 +/- 0.7524, Acurácia: 0.8839\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 457, Loss: 0.4545 +/- 0.8133, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 458, Loss: 0.3845 +/- 0.7672, Acurácia: 0.8903\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 458, Loss: 0.4564 +/- 0.7837, Acurácia: 0.8409\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 459, Loss: 0.3866 +/- 0.7333, Acurácia: 0.8796\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 459, Loss: 0.4271 +/- 0.7435, Acurácia: 0.8750\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 460, Loss: 0.3509 +/- 0.6970, Acurácia: 0.8937\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 460, Loss: 0.4197 +/- 0.7781, Acurácia: 0.8864\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 461, Loss: 0.3366 +/- 0.6509, Acurácia: 0.9002\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 461, Loss: 0.4054 +/- 0.7464, Acurácia: 0.8864\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 462, Loss: 0.3586 +/- 0.7092, Acurácia: 0.8936\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 462, Loss: 0.4302 +/- 0.7662, Acurácia: 0.8636\n",
      "\n",
      " ***************Treino***************\n",
      "Epoca: 463, Loss: 0.3533 +/- 0.6699, Acurácia: 0.8866\n",
      "\n",
      " ***************Teste***************\n",
      "Epoca: 463, Loss: 0.4387 +/- 0.8182, Acurácia: 0.8523\n"
     ]
    }
   ],
   "source": [
    "# Inicializa listas para registrar a loss média por época (treino e teste).\n",
    "loss_treino, loss_test = [], []\n",
    "# Inicializa listas para registrar a acurácia por época (treino e teste).\n",
    "acc_treino, acc_test = [], []\n",
    "\n",
    "# Amostra um conjunto fixo e pequeno de teste para avaliação a cada época.\n",
    "dados_test, rotulos_test = sample_batch(size=5)\n",
    "# Loop principal de treinamento por 1000 épocas (pode ser demorado).\n",
    "for epoca in range(1000):\n",
    "\n",
    "  # Para cada época, amostra um lote balanceado de treinamento.\n",
    "  dados_tns, rotulos_tns = sample_batch()\n",
    "  # Executa a passada de TREINO: retorna loss média e acurácia da época.\n",
    "  loss, acuracia = forward(dados_tns, rotulos_tns, 'Treino')\n",
    "  # Armazena a loss de treino desta época.\n",
    "  loss_treino.append(loss)\n",
    "  # Armazena a acurácia de treino desta época.\n",
    "  acc_treino.append(acuracia)  \n",
    "\n",
    "  # Executa a passada de TESTE no conjunto fixo (sem atualização de pesos).\n",
    "  loss, acuracia = forward(dados_test, rotulos_test, 'Teste')\n",
    "  # Armazena a loss de teste desta época.\n",
    "  loss_test.append(loss)\n",
    "  # Armazena a acurácia de teste desta época.\n",
    "  acc_test.append(acuracia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-jhpXs_z7vb"
   },
   "source": [
    "## Análise de Convergência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "executionInfo": {
     "elapsed": 1575,
     "status": "ok",
     "timestamp": 1605623194267,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "gM4BTf7lz7Q6",
    "outputId": "3f463882-5397-46b5-e98c-694d97a2c35c"
   },
   "outputs": [],
   "source": [
    "# Cria uma figura com 1 linha e 2 eixos (subplots) lado a lado; tamanho total de 16×4 polegadas.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,4))\n",
    "\n",
    "# Plota a curva de loss de treino (ignorando o primeiro ponto) no primeiro eixo.\n",
    "ax1.plot(loss_treino[1:], label='Train')\n",
    "# Plota a curva de loss de teste (ignorando o primeiro ponto) no primeiro eixo.\n",
    "ax1.plot(loss_test[1:], label='Test')\n",
    "# Define o título do gráfico do primeiro eixo.\n",
    "ax1.set_title('Model Convergence - Loss')\n",
    "# Rótulo do eixo X do primeiro gráfico (épocas).\n",
    "ax1.set_xlabel('epochs')\n",
    "# Rótulo do eixo Y do primeiro gráfico (loss).\n",
    "ax1.set_ylabel('Loss')\n",
    "# Exibe a legenda para distinguir as curvas de treino e teste no primeiro eixo.\n",
    "ax1.legend()\n",
    "\n",
    "# Plota a curva de acurácia de treino no segundo eixo.\n",
    "ax2.plot(acc_treino, label='Train')\n",
    "# Plota a curva de acurácia de teste no segundo eixo.\n",
    "ax2.plot(acc_test, label='Test')\n",
    "# Define o título do gráfico do segundo eixo.\n",
    "ax2.set_title('Model Convergence - Accuracy')\n",
    "# Rótulo do eixo X do segundo gráfico (épocas).\n",
    "ax2.set_xlabel('epochs')\n",
    "# Rótulo do eixo Y do segundo gráfico (acurácia).\n",
    "ax2.set_ylabel('Accuracy')\n",
    "# Exibe a legenda para distinguir as curvas de treino e teste no segundo eixo.\n",
    "ax2.legend()\n",
    "# Mostra a figura com os dois gráficos lado a lado.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dkx4uHTz-8l"
   },
   "source": [
    "## Usando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1605623359600,
     "user": {
      "displayName": "Camila Laranjeira",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gihu3QVmkuyR2Al5S_BxrjkrW8O_oUwd2ROuOwQerU=s64",
      "userId": "03895474106233302954"
     },
     "user_tz": 180
    },
    "id": "Rul7kSlY0Aui",
    "outputId": "9301be34-76f7-493f-8b40-d9f348cbbe26"
   },
   "outputs": [],
   "source": [
    "# Define uma função de inferência que recebe um nome (string) e imprime as 3 classes mais prováveis.\n",
    "def predict(nome):\n",
    "  # Coloca o modelo em modo de avaliação (desativa Dropout e usa BN com estatísticas fixas).\n",
    "  model.eval()\n",
    "\n",
    "  # Cria um tensor one-hot zerado com uma linha por caractere e uma coluna por símbolo do vocabulário.\n",
    "  tns = torch.zeros( len(nome), tam_dicionario )\n",
    "  # Percorre cada caractere do nome e marca a coluna correspondente no one-hot.\n",
    "  for k, letra in enumerate(nome):\n",
    "    # Busca o índice da letra no vocabulário; se não existir, retorna -1.\n",
    "    idx = caracteres_validos.find(letra)\n",
    "    # Só marca 1.0 se a letra for conhecida (idx >= 0); evita escrever na coluna -1 por engano.\n",
    "    if idx >= 0:                 # << evita o índice -1\n",
    "        tns[k, idx] = 1.0\n",
    "    # Caso contrário, mantém a linha toda zero (caractere ignorado).\n",
    "    # else: linha fica toda zero (caractere ignorado)\n",
    "  # Move o tensor de entrada para o dispositivo configurado (CPU/GPU).\n",
    "  tns = tns.to(args['device'])\n",
    "\n",
    "  # Forward: obtém as pontuações (aqui, log-probabilidades se o modelo usar LogSoftmax na saída).\n",
    "  saida = model(tns)\n",
    "  # Seleciona o top-3 ao longo da dimensão das classes (dim=1); retorna valores e índices.\n",
    "  topv, topi = saida.data.topk(3, 1, True)\n",
    "\n",
    "  # Imprime o nome consultado.\n",
    "  print(nome)\n",
    "  # Itera pelos pares (valor, índice) do top-3 para exibir a classe prevista.\n",
    "  # OBS: `index` é um tensor escalar; se necessário para indexação em listas, use `index.item()`.\n",
    "  for value, index in zip(topv[0], topi[0]):\n",
    "    # Mostra a pontuação (value) e a classe correspondente em `categorias`.\n",
    "    print('(%.2f) %s' % (value, categorias[index]))\n",
    "  # Linha em branco para separar chamadas.\n",
    "  print('\\n')\n",
    "\n",
    "# Exemplos de uso da função de predição com três nomes.\n",
    "predict('Merkel')\n",
    "predict('Hirobumi')\n",
    "predict('Suarez')\n",
    "predict('Ow-Gao')\n",
    "predict('De la mancha')\n",
    "predict('Lessa')\n",
    "predict('Leça')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo treinado em disco\n",
    "torch.save(model.state_dict(), 'rnn_gru_name_classifier.pth')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM4D023QOGnkkQJXatAtmLz",
   "collapsed_sections": [],
   "name": "Classificação de Sequências-GRU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
